import logging
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import google.generativeai as genai
import re
import os
import datetime
from Cache_code import load_from_cache,append_to_cache,save_to_cache
from datetime import datetime
import json
import variables

from variables import EMAIL_HOST, EMAIL_PORT, EMAIL_USER, EMAIL_PASSWORD, TEAM_EMAIL




def extract_client_preferences(user_message):

    extracted_info = {
        "property_preferences": "",
        "budget": 0,
        "location": "",
        "property_type": "",
        "bedrooms": 0,
        "bathrooms": 0
    }

    # Extract budget
    budget_match = re.search(r"(\d+(?:,\d{3})*)\s*(Ø¯ÙˆÙ„Ø§Ø±|Ù…Ù„ÙŠÙˆÙ†|Ø£Ù„Ù|Ø¬Ù†ÙŠÙ‡)", user_message)
    if budget_match:
        amount = int(budget_match.group(1).replace(",", ""))
        unit = budget_match.group(2)
        extracted_info["budget"] = amount * 1_000_000 if unit == "Ù…Ù„ÙŠÙˆÙ†" else amount * 1_000

    # Extract location (e.g., Ø§Ù„ØªØ¬Ù…Ø¹ Ø§Ù„Ø®Ø§Ù…Ø³, 6 Ø£ÙƒØªÙˆØ¨Ø±)
    locations = ["Ø§Ù„Ø¹Ø¨ÙˆØ±","Ø§Ù„Ø±ÙŠØ§Ø¶","Ø¯Ø¨ÙŠ ","Ø§Ù„ØªØ¬Ù…Ø¹ Ø§Ù„Ø®Ø§Ù…Ø³","Ù…Ø¯ÙŠÙ†Ø© Ù†ØµØ±","Ù…ØµØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ù‡","Ø§Ù„Ù…Ø¹Ø§Ø¯ÙŠ","Ø§Ù„Ø±Ø­Ø§Ø¨","Ø§Ù„Ø´ÙŠØ® Ø²Ø§ÙŠØ¯"," Ù…Ø¯ÙŠÙ†ØªÙŠ", "6 Ø£ÙƒØªÙˆØ¨Ø±","Ø§Ù„Ø¹Ø§ØµÙ…Ù‡ Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠÙ‡", "Ø§Ù„Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ø¥Ø¯Ø§Ø±ÙŠØ©", "Ø§Ù„Ø³Ø§Ø­Ù„ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠ"]
    for loc in locations:
        if loc in user_message:
            extracted_info["location"] = loc
            break

    # Extract property type (e.g., Ø´Ù‚Ø©, ÙÙŠÙ„Ø§)
    property_types = ["ØªØ¬Ø§Ø±ÙŠ","Ù…Ø·Ø¹Ù…","Ø¹ÙŠØ§Ø¯Ù‡","Ù…Ø­Ù„","Ø´Ù‚Ù‡","Ø´Ù‚Ø©", "ÙÙŠÙ„Ø§", "Ø¯ÙˆØ¨Ù„ÙƒØ³", "Ø¨Ù†ØªÙ‡Ø§ÙˆØ³"]
    for ptype in property_types:
        if ptype in user_message:
            extracted_info["property_type"] = ptype
            break

    # Extract bedrooms
    bedrooms_match = re.search(r"(\d+)\s*ØºØ±Ù", user_message)
    if bedrooms_match:
        extracted_info["bedrooms"] = int(bedrooms_match.group(1))

    # Extract bathrooms
    bathrooms_match = re.search(r"(\d+)\s*Ø­Ù…Ø§Ù…", user_message)
    if bathrooms_match:
        extracted_info["bathrooms"] = int(bathrooms_match.group(1))

    return extracted_info


def send_email(to_email, subject, body):
    """
    Send an email to the specified address.
    """
    msg = MIMEMultipart()
    msg['From'] = EMAIL_USER
    msg['To'] = to_email
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'plain'))

    try:
        print(f"ğŸ“§ Attempting to send email to: {to_email}")
        print(f"ğŸ“§ Subject: {subject}")
        server = smtplib.SMTP(EMAIL_HOST, EMAIL_PORT)
        server.starttls()
        server.login(EMAIL_USER, EMAIL_PASSWORD)
        server.sendmail(EMAIL_USER, to_email, msg.as_string())
        server.quit()
        print(f"âœ… Email sent successfully to: {to_email}")
        return True
    except Exception as e:
        print(f"âŒ Failed to send email: {e}")
        return False
from session_store import get_session

def schedule_viewing(arguments):
    logging.info(f"Received arguments: {arguments}")

    # Get conversation_id (aka thread_id)
    conversation_id = arguments.get('conversation_id')
    session_info = get_session(conversation_id) if conversation_id else {}

    # Prioritize data from arguments, fallback to session
    client_id = arguments.get('client_id') or session_info.get('user_id')
    name = arguments.get('name') or session_info.get('name', 'Unknown')
    phone = arguments.get('phone') or session_info.get('phone', 'Unknown')
    email = arguments.get('email') or session_info.get('email', 'Unknown')

    property_id = arguments.get('property_id', 'Not Provided')
    desired_date = arguments.get('desired_date')
    desired_time = arguments.get('desired_time')
    meeting_type = arguments.get('meeting_type')

    if not desired_date or not desired_time or not meeting_type:
        return {
            "message": "ğŸ“… Ù…Ù† ÙØ¶Ù„Ùƒ Ø§Ø®ØªØ± ØªØ§Ø±ÙŠØ® ÙˆÙˆÙ‚Øª Ù„Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©ØŒ ÙˆÙ‡Ù„ ØªÙØ¶Ù„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ Ø¹Ø¨Ø± Ø²ÙˆÙˆÙ… Ø£Ù… Ø²ÙŠØ§Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ©ØŸ"
        }

    developer_name = get_developer_name_from_database(property_id) or "Unknown Developer"
    property_name = get_property_name_from_database(property_id) or "Unknown Property"

    summary = advanced_conversation_summary_from_db(client_id, conversation_id, name, property_id)

    subject = f"ğŸ”” Ù…Ø¹Ø§ÙŠÙ†Ø© ÙˆØ­Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø© - ID {property_id}"
    body = f"""
    ğŸ“ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„:
    - client id : {client_id}
    - Name : {name}
    - Phone : {phone}
    - Email: {email}
    - property_id :{property_id}
    - Unit Name: {property_name}
    - Devloper : {developer_name}
    - Meeting type: {meeting_type}
    - Date : {desired_date}
    - Time : {desired_time}
    
    # ğŸ” Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
    {summary}
    """

    logging.info(f"Prepared email body: {body}")
    print(f"ğŸ“§ Sending appointment email to: {TEAM_EMAIL}")
    import threading
    threading.Thread(target=send_email, args=(TEAM_EMAIL, subject, body)).start()

    return {
        "message": "âœ… ØªÙ… Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ø¨Ù†Ø¬Ø§Ø­!",
        "client_id": client_id,
        "property_id": property_id,
        "developer": developer_name,
        "date": desired_date,
        "time": desired_time,
        "meeting_type": meeting_type,
    }
# def retrieve_lead_info(client_id):
#     """Retrieve client details from the leads table in MySQL instead of Excel."""
#     if not client_id:
#         return None
#
#     query = "SELECT * FROM leads WHERE user_id = %s"
#     params = (client_id,)
#     result = db_operations.fetch_data(query, params)
#
#     if result:
#         print("âœ… Lead info fetched from MySQL")
#         return result[0]  # Return first matched lead as a dictionary
#
#     print("âŒ Lead not found in MySQL")
#     return None

def get_developer_name_from_database(property_id):
    try:
        units = load_from_cache("units.json")
        for unit in units:
            if str(unit.get("id")) == str(property_id):
                return unit.get("developer_name", "Unknown Developer")
    except Exception as e:
        print(f"âš ï¸ Error loading developer name from cache: {e}")
    return "Unknown Developer"


def get_property_name_from_database(property_id):
    try:
        units = load_from_cache("units.json")
        for unit in units:
            if str(unit.get("id")) == str(property_id):
                return unit.get("name_ar", "Unknown Property")
    except Exception as e:
        print(f"âš ï¸ Error loading property name from cache: {e}")
    return "Unknown Property"

def search_new_launches(arguments):
    """
    Search new launches via Chroma semantic search only (no metadata filters).
    Returns up to 50 results ranked by semantic relevance.
    """
    print("ğŸš€ search_new_launches function called!")
    print(f"ğŸš€ Arguments: {arguments}")
    
    # Test if function is being called at all
    if not arguments:
        return {"error": "No arguments provided"}
    
    try:
        # Build semantic query from arguments (text-only)
        property_type = str(arguments.get("property_type", "")).strip().lower()
        location = str(arguments.get("location", "")).strip().lower()
        compound = str(arguments.get("compound", "")).strip().lower()

        semantic_parts = []
        if property_type:
            semantic_parts.append(property_type)
        if location:
            semantic_parts.append(location)
        if compound:
            semantic_parts.append(compound)
        
        query_text = " ".join(semantic_parts) if semantic_parts else "new launch properties"

        # Use RAG (Chroma) semantic search for new launches
        from chroma_rag_setup import get_rag_instance
        rag = get_rag_instance()
        rag_results = rag.search_new_launches(query_text, n_results=50, filters=None)

        if not rag_results:
                return {
                "source": "chromadb_semantic_search_new_launches",
                "message": f"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¥Ø·Ù„Ø§Ù‚Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù…ÙˆØ§ØµÙØ§ØªÙƒ{(' ÙÙŠ ' + location) if location else ''}.",
                    "results": []
                }
            
        # Format results for clean UI (ID | Name | City | Compound)
        def _format_launch(item, idx):
            meta = item.get('metadata', {})
            doc = item.get('document', '')
            
            # Extract information from document text
            name_val = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
            city_val = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
            compound_val = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            # Extract name from document
            if 'Name (AR):' in doc:
                name_val = doc.split('Name (AR):')[1].split('\n')[0].strip()
            elif 'Name (EN):' in doc:
                name_val = doc.split('Name (EN):')[1].split('\n')[0].strip()
            
            # Extract city from document
            if 'City:' in doc:
                city_val = doc.split('City:')[1].split('\n')[0].strip()
            
            # Extract compound from document
            if 'Compound (AR):' in doc:
                compound_val = doc.split('Compound (AR):')[1].split('\n')[0].strip()
            elif 'Compound (EN):' in doc:
                compound_val = doc.split('Compound (EN):')[1].split('\n')[0].strip()
            
            # Get launch_id from metadata
            launch_id = meta.get('launch_id', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
            
            return f"{idx}. ID:{launch_id} | {name_val} | Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©: {city_val} | Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯: {compound_val}"

        # Format all results first
        all_formatted_lines = [_format_launch(itm, i+1) for i, itm in enumerate(rag_results[:10])]
        
        # Apply LLM re-filtering to keep only most relevant results
        def _llm_filter_new_launches(lines, prop_type, loc):
            print("ğŸ” ENTERING LLM FILTER FUNCTION")
            print(f"ğŸ” LLM filter function called with {len(lines)} lines")
            try:
                if not lines or len(lines) <= 3:
                    print(f"ğŸ” Skipping LLM filter - only {len(lines)} results")
                    return lines  # Keep all if 3 or fewer results
                
                # Build prompt for LLM filtering
                numbered = "\n".join([f"{i+1}. {line}" for i, line in enumerate(lines)])
                intent_desc = f"property_type='{prop_type}' | location='{loc}'"
                
                prompt = f"""
                Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¹Ù‚Ø§Ø±ÙŠ Ø°ÙƒÙŠ. Ù‚Ù… Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ø¥Ø·Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ (New Launches).
                
                Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: Ø§Ø®ØªØ± ÙÙ‚Ø· Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø± ÙˆØ§Ù„Ù…ÙˆÙ‚Ø¹).
                Ù†ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ„: {intent_desc}
                
                Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©:
                {numbered}
                
                Ø§Ø±Ø¬Ø¹ ÙÙ‚Ø· JSON array Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¹Ù†Ø§ØµØ± (1-based indices) Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¢Ø®Ø±.
                Ù…Ø«Ø§Ù„: [1,3,5]
                
                Ø§Ø®ØªØ± 3-4 Ø¹Ù†Ø§ØµØ± ÙÙ‚Ø· Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø©.
                """
                
                print(f"ğŸ¤– Sending LLM prompt: {prompt[:200]}...")
                
                # Use Gemini for filtering
                import os
                import google.generativeai as genai
                from variables import GEMINI_API_KEY
                
                genai.configure(api_key=GEMINI_API_KEY)
                model = genai.GenerativeModel('gemini-2.0-flash-exp')
                
                response = model.generate_content(prompt)
                text = (response.text or "").strip()
                
                print(f"ğŸ¤– LLM response: {text}")
                
                # Extract JSON array
                import json
                start = text.find('[')
                end = text.rfind(']')
                indices = []
                if start != -1 and end != -1 and end > start:
                    json_part = text[start:end+1]
                    try:
                        indices = json.loads(json_part)
                        print(f"ğŸ” Parsed indices: {indices}")
                    except Exception as e:
                        print(f"ğŸš¨ JSON parsing failed: {e}")
                        indices = []
                
                # Map to lines
                filtered = []
                for idx in indices:
                    try:
                        i = int(idx) - 1
                        if 0 <= i < len(lines):
                            filtered.append(lines[i])
                    except Exception:
                        continue
            
                print(f"ğŸ” Filtered to {len(filtered)} results")
                
                # Fallback if empty - return top 3
                return filtered if filtered else lines[:3]
                
            except Exception as e:
                print(f"ğŸš¨ LLM filter failed: {e}")
                # Fallback to top 3 results
                return lines[:3]
        
        # Apply LLM filtering
        print(f"ğŸ” Applying LLM filtering for property_type='{property_type}', location='{location}'")
        print(f"ğŸ“Š Before filtering: {len(all_formatted_lines)} results")
        
        # Simple test - just return top 3 for now to verify the flow works
        if len(all_formatted_lines) > 3:
            print("ğŸ” More than 3 results, applying LLM filtering...")
            filtered_lines = _llm_filter_new_launches(all_formatted_lines, property_type, location)
        else:
            print("ğŸ” 3 or fewer results, keeping all")
            filtered_lines = all_formatted_lines
        
        print(f"ğŸ“Š After filtering: {len(filtered_lines)} results")
        
        # Compute similarity-like scores from chroma distance (not shown in UI)
        launch_similarity_scores = []
        for itm in rag_results[:len(filtered_lines)]:
            try:
                dist = float(itm.get('distance', 0))
                sim = 1.0 - dist
                if sim < 0:
                    sim = 0.0
                if sim > 1:
                    sim = 1.0
                launch_similarity_scores.append(round(sim, 4))
            except Exception:
                launch_similarity_scores.append(None)
        
            return {
            "source": "chromadb_semantic_search_new_launches",
            "message": f"âœ… Ù„Ù‚ÙŠØª {len(filtered_lines)} Ø¥Ø·Ù„Ø§Ù‚Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù…ÙˆØ§ØµÙØ§ØªÙƒ ğŸ‘‡",
            "results": filtered_lines,
            "follow_up": "Ù„Ùˆ Ø­Ø§Ø¨Ø¨ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ± Ø¹Ù† Ø¥Ø·Ù„Ø§Ù‚ Ù…Ø¹ÙŠÙ† Ø§Ø¨Ø¹ØªÙ„ÙŠ Ø±Ù‚Ù… Ø§Ù„Ù€ ID ğŸ”",
            "similarity_scores": launch_similarity_scores
        }

    except Exception as e:
        print(f"ğŸš¨ Error in search_new_launches (RAG): {e}")
        return {
            "error": f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©: {str(e)}"
        }


def create_lead(data):
    user_id = data.get("user_id")
    if not user_id:
        return {"success": False, "message": "User ID is required."}

    # 1ï¸âƒ£ Load all leads from daily cache
    cached_leads = load_from_cache("leads_cache.json")
    user_lead = next((lead for lead in cached_leads if str(lead.get("user_id")) == str(user_id)), None)

    fields = ["name", "phone", "email", "property_preferences", "budget", "location", "property_type", "bedrooms", "bathrooms"]

    if user_lead:
        # 2ï¸âƒ£ Update logic: only update changed fields
        updated = False
        for field in fields:
            new_val = data.get(field)
            if new_val and new_val != user_lead.get(field):
                user_lead[field] = new_val
                updated = True

        if updated:
            # Update cached leads in memory
            for i, lead in enumerate(cached_leads):
                if str(lead.get("user_id")) == str(user_id):
                    cached_leads[i] = user_lead
                    break

            # Save updated leads cache
            from Cache_code import save_to_cache
            save_to_cache("leads_cache.json", cached_leads)

            # Also append to update queue for later DB sync
            append_to_cache("leads_updates.json", user_lead)
            logging.info(f"âœ… Lead updated in cache and queued for DB update: {user_id}")
        else:
            logging.info(f"âš ï¸ No changes to update for cached lead: {user_id}")
    else:
        # 3ï¸âƒ£ New lead: append to both caches
        new_lead = {
            "user_id": user_id,
            "name": data.get("name", ""),
            "phone": data.get("phone", ""),
            "email": data.get("email", ""),
            "property_preferences": data.get("property_preferences", ""),
            "budget": data.get("budget", 0),
            "location": data.get("location", ""),
            "property_type": data.get("property_type", ""),
            "bedrooms": data.get("bedrooms", 0),
            "bathrooms": data.get("bathrooms", 0)
        }
        append_to_cache("leads_cache.json", new_lead)
        append_to_cache("leads_updates.json", new_lead)
        logging.info(f"ğŸ†• New lead cached and queued for DB insert: {user_id}")

    return {"success": True, "message": "Lead cached successfully."}
#
# def get_city_aliases_from_db():
#     query = "SELECT id, name_en, name_ar FROM cities"
#     cities = db_operations.fetch_data(query)
#     city_map = {}
#
#     for city in cities:
#         # Create a keyword dictionary entry with various lowercase aliases
#         aliases = [
#             city['name_en'].lower(),
#             city['name_ar'].lower(),
#             city['name_en'].lower().replace("city", "").strip(),
#             city['name_en'].lower().replace(" ", ""),
#             city['name_en'].lower().replace("october", "6th of october"),
#             city['name_en'].lower().replace("october", "6 october"),
#         ]
#         aliases = list(set(aliases))  # remove duplicates
#         city_map[city['id']] = aliases
#     return city_map
#
# def find_city_ids_for_location(location_input, city_map):
#     location_input = location_input.strip().lower()
#     matched_city_ids = []
#
#     for city_id, aliases in city_map.items():
#         for alias in aliases:
#             if alias in location_input:
#                 matched_city_ids.append(city_id)
#                 break  # stop checking aliases for this city once matched
#
#     return matched_city_ids
#
def clean_price_string(price_str):
    if price_str is None:
        return 0.0
    try:
        return float(str(price_str).replace(",", "").replace("\xa0", "").strip())
    except ValueError:
        return 0.0

def mmr_search(results, query_embedding, lambda_param=0.8, top_k=10):
    """
    Implement Maximal Marginal Relevance (MMR) search to diversify results.
    
    Args:
        results (list): List of property results
        query_embedding (list): Query embedding vector
        lambda_param (float): Balance between relevance (Î») and diversity (1-Î»)
        top_k (int): Number of top results to return
    
    Returns:
        list: Diversified results using MMR
    """
    try:
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity
        
        if not results or len(results) <= 1:
            return results
        
        # Simple text-based similarity calculation (fallback when embeddings aren't available)
        def calculate_similarity(item1, item2):
            """Calculate similarity between two property items based on text features"""
            features1 = f"{item1.get('compound_name', '')} {item1.get('name_ar', '')} {item1.get('location', '')}"
            features2 = f"{item2.get('compound_name', '')} {item2.get('name_ar', '')} {item2.get('location', '')}"
            
            # Convert to lowercase and split into words
            words1 = set(features1.lower().split())
            words2 = set(features2.lower().split())
            
            # Calculate Jaccard similarity
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            return intersection / union if union > 0 else 0
        
        def calculate_query_similarity(item):
            """Calculate similarity between query and property item"""
            query_features = f"{item.get('compound_name', '')} {item.get('name_ar', '')} {item.get('location', '')}"
            query_words = set(query_features.lower().split())
            
            # Simple relevance score based on feature matching
            relevance_score = 0
            if item.get('location'):
                relevance_score += 0.3
            if item.get('compound_name'):
                relevance_score += 0.2
            if item.get('name_ar'):
                relevance_score += 0.2
            if item.get('price'):
                relevance_score += 0.1
            if item.get('Bedrooms'):
                relevance_score += 0.1
            if item.get('delivery_type'):
                relevance_score += 0.1
            
            return relevance_score
        
        # Initialize MMR selection
        selected_indices = []
        remaining_indices = list(range(len(results)))
        
        # Select first item (most relevant)
        first_item_idx = max(remaining_indices, key=lambda i: calculate_query_similarity(results[i]))
        selected_indices.append(first_item_idx)
        remaining_indices.remove(first_item_idx)
        
        # Select remaining items using MMR
        for _ in range(min(top_k - 1, len(remaining_indices))):
            mmr_scores = []
            
            for idx in remaining_indices:
                # Relevance score
                relevance = calculate_query_similarity(results[idx])
                
                # Diversity score (average similarity to already selected items)
                diversity = 0
                if selected_indices:
                    similarities = [calculate_similarity(results[idx], results[sel_idx]) for sel_idx in selected_indices]
                    diversity = np.mean(similarities)
                
                # MMR score
                mmr_score = lambda_param * relevance - (1 - lambda_param) * diversity
                mmr_scores.append((idx, mmr_score))
            
            # Select item with highest MMR score
            best_idx = max(mmr_scores, key=lambda x: x[1])[0]
            selected_indices.append(best_idx)
            remaining_indices.remove(best_idx)
        
        # Return results in MMR order
        return [results[i] for i in selected_indices]
        
    except Exception as e:
        print(f"ğŸš¨ Error in MMR search: {e}")
        # Fallback to original results
        return results[:top_k]


def property_search(arguments):
    """
    Search for existing property units using ChromaDB with MMR search and semantic search.
    This function does NOT use cache - it searches directly from ChromaDB.
    """
    try:
        # Mandatory fields
        location = arguments.get("location", "").strip().lower()
        budget = arguments.get("budget")
        
        # Optional fields
        property_type = arguments.get("property_type", "").strip().lower()
        bedrooms = arguments.get("bedrooms")
        bathrooms = arguments.get("bathrooms")
        apartment_area = arguments.get("apartment_area")
        # Optional preferred compound (accept both 'compound' and 'compound_name')
        compound_name = (arguments.get("compound") or arguments.get("compound_name") or "").strip()
        
        # Progressive search parameters
        price_tolerance = arguments.get("price_tolerance", 0)  # 0%, 10%, 20%, 25%
        excluded_ids = arguments.get("excluded_ids", [])  # IDs to exclude from results
        search_round = arguments.get("search_round", 1)  # 1st, 2nd, or 3rd round

        # Enforce required fields for existing units flow
        missing_fields = []
        if not location:
            missing_fields.append("Ø§Ù„Ù…ÙˆÙ‚Ø¹")
        # Keep original value for validation before normalization
        raw_budget = budget
        if raw_budget in (None, "", 0):
            missing_fields.append("Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©")
        if not property_type:
            missing_fields.append("Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±")

        if missing_fields:
            return {
                "source": "validation",
                "message": "Ù…Ø­ØªØ§Ø¬ Ù…Ù†Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© Ù‚Ø¨Ù„ Ù…Ø§ Ø£Ø¨Ø¯Ø£ Ø§Ù„Ø¨Ø­Ø«: " + ", ".join(missing_fields) + ".\n" \
                           + "- Ù…Ø«Ø§Ù„ Ù„Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©: 4,000,000 Ø£Ùˆ 4 Ù…Ù„ÙŠÙˆÙ†\n" \
                           + "- Ù…Ù…ÙƒÙ† ÙƒÙ…Ø§Ù† ØªÙ‚ÙˆÙ„Ù‘ÙŠ ÙƒÙ…Ø¨ÙˆÙ†Ø¯ Ù…ÙØ¶Ù„ Ù„Ùˆ Ø­Ø§Ø¨Ø¨ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)",
                "results": []
            }
        
        # Convert budget to float if it's a string
        try:
            budget = float(budget) if budget else 0
        except ValueError:
            budget = 0
        
        # Apply price tolerance for progressive search
        if price_tolerance > 0:
            tolerance_factor = 1 + (price_tolerance / 100)
            max_budget = budget * tolerance_factor
            min_budget = budget / tolerance_factor
        else:
            # First search: 10% tolerance
            tolerance_factor = 1.1  # 10%
            max_budget = budget * tolerance_factor
            min_budget = budget / tolerance_factor
        
        # Build search query for ChromaDB semantic search
        search_query_parts = []
        if location:
            search_query_parts.append(f"location: {location}")
        if property_type:
            search_query_parts.append(f"property type: {property_type}")
        if compound_name:
            search_query_parts.append(f"compound: {compound_name}")
        if budget > 0:
            search_query_parts.append(f"budget around {budget}")
        if bedrooms:
            search_query_parts.append(f"{bedrooms} bedrooms")
        if bathrooms:
            search_query_parts.append(f"{bathrooms} bathrooms")
        if apartment_area:
            search_query_parts.append(f"area around {apartment_area}")
        
        # Create semantic search query
        search_query = " ".join(search_query_parts) if search_query_parts else "property units"
        
        try:
            # Import and use the proper ChromaDB RAG system
            from chroma_rag_setup import get_rag_instance
            # MMR is now handled directly in the RAG pipeline
            
            # Initialize RAG system
            rag = get_rag_instance()
            
            # Build filters for ChromaDB query
            filters = {}
            # Apply early price filter with 10% tolerance (or exact range if provided)
            if budget and budget > 0:
                budget_str = str(arguments.get("budget", ""))
                if "-" in budget_str and any(unit in budget_str.lower() for unit in ["Ù…Ù„ÙŠÙˆÙ†", "million", "m"]):
                    import re
                    m = re.search(r"(\d+)\s*-\s*(\d+)", budget_str)
                    if m:
                        min_val = int(m.group(1)) * 1_000_000
                        max_val = int(m.group(2)) * 1_000_000
                        filters["price_min"] = min_val
                        filters["price_max"] = max_val
                else:
                    filters["price_min"] = int(min_budget)
                    filters["price_max"] = int(max_budget)
            # Pass semantic hints for reranker (not used in Chroma where clause)
            if location:
                filters["query_location"] = location
            if property_type:
                filters["query_property_type"] = property_type
            if compound_name:
                filters["query_compound"] = compound_name
            
            # Perform semantic search using ChromaDB with MMR
            # Get more results initially for MMR processing
            initial_results = rag.search_units(search_query, n_results=50, filters=filters)
            
            if not initial_results:
                return {
                    "source": "no_relevant_results",
                    "message": "Ø¢Ø³Ù ÙŠØ§ ÙÙ†Ø¯Ù…ØŒ Ù…Ø´ Ù„Ø§Ù‚ÙŠ ÙˆØ­Ø¯Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ù„ÙŠ Ø·Ù„Ø¨ØªÙ‡Ø§ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ.\n\n" \
                               "ØªÙ‚Ø¯Ø± ØªØªÙˆØ§ØµÙ„ Ù…Ø¹ ÙØ±ÙŠÙ‚ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø­Ø¯Ø« Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ØªØ§Ø­Ø©:\n" \
                               "ğŸ“± ÙˆØ§ØªØ³Ø§Ø¨: https://wa.me/201000730208",
                    "results": []
                }
            
            # Convert ChromaDB results to the format expected by the system
            formatted_results = []
            for result in initial_results:
                # Extract the document content and metadata
                doc_content = result.get('document', '')
                metadata = result.get('metadata', {})
                
                # Create a result item that matches the expected format
                result_item = {
                    'id': metadata.get('unit_id', 'unknown'),
                    'name_en': doc_content,  # Use the document content
                    'name_ar': doc_content,  # Use the document content for Arabic too
                    'compound_name': metadata.get('compound_name', ''),
                    'location': metadata.get('location', ''),
                    'property_type': metadata.get('property_type', ''),
                    'price': metadata.get('price_value', 0),
                    'bedrooms': metadata.get('bedrooms', 0),
                    'bathrooms': metadata.get('bathrooms', 0),
                    'apartment_area': metadata.get('apartment_area', 0),
                    'delivery_in': metadata.get('delivery_in', ''),
                    'installment_years': metadata.get('installment_years', ''),
                    'new_image': metadata.get('new_image', ''),
                    'chroma_score': result.get('distance', 0)
                }
                formatted_results.append(result_item)
            
            # Results are already MMR-optimized from the RAG pipeline
            diversified_results = formatted_results
            
            # Apply additional filtering based on search criteria
            filtered_results = []
            
            def check_price_match(budget, arguments, item):
                """Helper function to check if price matches budget with proper tolerance/ranges"""
                try:
                    price = float(item.get("price", 0) or 0)
                    budget_str = str(arguments.get("budget", ""))
                    if "-" in budget_str and any(unit in budget_str.lower() for unit in ["Ù…Ù„ÙŠÙˆÙ†", "million", "m"]):
                        import re
                        range_match = re.search(r"(\d+)\s*-\s*(\d+)", budget_str)
                        if range_match:
                            min_val = int(range_match.group(1)) * 1_000_000
                            max_val = int(range_match.group(2)) * 1_000_000
                            return min_val <= price <= max_val
                    # Use dynamic tolerance based on search round
                    return min_budget <= price <= max_budget
                except Exception:
                    return False
            
            for item in diversified_results:
                # Exclude previously shown units for progressive search
                item_id = str(item.get('id', ''))
                if item_id in excluded_ids:
                    continue
                
                # Apply price filtering since client needs specific budget
                if budget and budget > 0:
                    if not check_price_match(budget, arguments, item):
                        continue

                # Location and property type matching is handled by semantic search (embeddings)
                # No additional filtering needed as ChromaDB's semantic search handles this accurately

                filtered_results.append(item)
            
            # -------------------- Format Results for UI --------------------
            def format_unit(item, index):
                # Extract a clean Arabic name and basic meta
                doc_text = item.get('name_ar', '') or item.get('name_en', '')
                name_ar = ""
                if 'Name (AR):' in doc_text:
                    name_ar = doc_text.split('Name (AR):')[1].split('\n')[0].strip()
                if not name_ar:
                    name_ar = doc_text.split('\n')[0][:120]
                price_val = item.get('price', 0)
                price_str = f"{int(price_val):,}" if isinstance(price_val, (int, float)) and price_val else "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                bedrooms = item.get('bedrooms', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                bathrooms = item.get('bathrooms', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                unit_id = item.get('id', 'ØºÙŠØ± Ù…ØªÙˆÙØ±') or item.get('unit_id', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                return f"{index}. ID:{unit_id} | {name_ar} | Ø§Ù„Ø³Ø¹Ø±: {price_str} EGP | ØºØ±Ù: {bedrooms} | Ø­Ù…Ø§Ù…: {bathrooms}"
            
            if not filtered_results:
                return {
                    "source": "no_relevant_results",
                    "message": "Ø¢Ø³Ù ÙŠØ§ ÙÙ†Ø¯Ù…ØŒ Ù…Ø´ Ù„Ø§Ù‚ÙŠ ÙˆØ­Ø¯Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ù„ÙŠ Ø·Ù„Ø¨ØªÙ‡Ø§ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ.\n\n" \
                               "ØªÙ‚Ø¯Ø± ØªØªÙˆØ§ØµÙ„ Ù…Ø¹ ÙØ±ÙŠÙ‚ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø­Ø¯Ø« Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ØªØ§Ø­Ø©:\n" \
                               "ğŸ“± ÙˆØ§ØªØ³Ø§Ø¨: https://wa.me/201000730208",
                    "results": []
                }
            
            # Quality check: Ensure results are truly relevant
            quality_filtered_results = []
            for result in filtered_results:
                # Check if result has minimum quality indicators
                result_text = str(result.get('name_ar', '') or result.get('name_en', '') or result.get('document', '')).lower()
                
                # Location quality check - must contain location reference
                location_match = False
                if location:
                    location_terms = [location.lower(), location.replace('Ø§Ù„', '').lower()]
                    location_match = any(term in result_text for term in location_terms if term)
                else:
                    location_match = True  # No location specified, skip check
                
                # Price quality check - must be reasonable
                price_match = True  # We already filtered by price earlier
                
                # Only include if quality checks pass
                if location_match and price_match:
                    quality_filtered_results.append(result)
            
            # Final quality check: If we have very few quality results, apologize instead
            if len(quality_filtered_results) < 3:
                return {
                    "source": "low_quality_results",
                    "message": "Ø¢Ø³Ù ÙŠØ§ ÙÙ†Ø¯Ù…ØŒ Ù…Ø´ Ù„Ø§Ù‚ÙŠ ÙˆØ­Ø¯Ø§Øª ÙƒØ§ÙÙŠØ© ØªØ·Ø§Ø¨Ù‚ Ù…ÙˆØ§ØµÙØ§ØªÙƒ Ø¨Ø¯Ù‚Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ.\n\n" \
                               "ØªÙ‚Ø¯Ø± ØªØªÙˆØ§ØµÙ„ Ù…Ø¹ ÙØ±ÙŠÙ‚ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø­Ø¯Ø« Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ØªØ§Ø­Ø©:\n" \
                               "ğŸ“± ÙˆØ§ØªØ³Ø§Ø¨: https://wa.me/201000730208",
                    "results": []
                }
            
            # Prepare formatted list (max 10)
            formatted_lines = [format_unit(itm, i+1) for i, itm in enumerate(quality_filtered_results[:10])]
            
            # Analyze actual compounds found in results to generate accurate message
            def generate_accurate_intro_message(results, location, requested_compound):
                """Generate intro message based on actual results, not requested criteria"""
                try:
                    # Extract unique compounds from actual results
                    found_compounds = set()
                    for result in results[:10]:  # Only check top 10 displayed results
                        compound_ar = result.get('compound_name_ar', '').strip()
                        compound_en = result.get('compound_name_en', '').strip()
                        compound_name = compound_ar or compound_en
                        if compound_name and compound_name.lower() != 'ØºÙŠØ± Ù…ØªÙˆÙØ±':
                            found_compounds.add(compound_name)
                    
                    # Generate message based on what was actually found
                    location_text = location if location else 'Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©'
                    
                    if len(found_compounds) == 0:
                        compound_text = "Ø¯Ø§Ø®Ù„ ÙƒÙ…Ø¨ÙˆÙ†Ø¯Ø§Øª Ù…Ø®ØªÙ„ÙØ©"
                    elif len(found_compounds) == 1:
                        compound_text = f"Ø¯Ø§Ø®Ù„ ÙƒÙ…Ø¨ÙˆÙ†Ø¯ {list(found_compounds)[0]}"
                    elif len(found_compounds) <= 3:
                        compound_list = list(found_compounds)[:3]
                        compound_text = f"Ø¯Ø§Ø®Ù„ ÙƒÙ…Ø¨ÙˆÙ†Ø¯Ø§Øª {' Ùˆ '.join(compound_list)}"
                    else:
                        compound_text = "Ø¯Ø§Ø®Ù„ ÙƒÙ…Ø¨ÙˆÙ†Ø¯Ø§Øª Ù…Ø®ØªÙ„ÙØ©"
                    
                    return (
                        f"ğŸ‘‹ Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ø­Ø¶Ø±ØªÙƒ!\nÙ„Ù‚ÙŠØªÙ„Ùƒ ÙˆØ­Ø¯Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø© ÙÙŠ {location_text} "
                        f"{compound_text}ØŒ ÙˆÙÙŠ Ø­Ø¯ÙˆØ¯ Ù…ÙŠØ²Ø§Ù†ÙŠØªÙƒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù ÙˆØ§Ù„Ø­Ù…Ø§Ù…Ø§Øª Ø§Ù„Ù„ÙŠ Ø·Ù„Ø¨ØªÙ‡Ù… ğŸ‘‡"
                    )
                except Exception as e:
                    logging.warning(f"Error generating intro message: {e}")
                    # Fallback to simple message without compound mention
                    return (
                        f"ğŸ‘‹ Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ø­Ø¶Ø±ØªÙƒ!\nÙ„Ù‚ÙŠØªÙ„Ùƒ ÙˆØ­Ø¯Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø© ÙÙŠ {location if location else 'Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©'} "
                        "Ø¯Ø§Ø®Ù„ ÙƒÙ…Ø¨ÙˆÙ†Ø¯Ø§Øª Ù…Ø®ØªÙ„ÙØ©ØŒ ÙˆÙÙŠ Ø­Ø¯ÙˆØ¯ Ù…ÙŠØ²Ø§Ù†ÙŠØªÙƒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù ÙˆØ§Ù„Ø­Ù…Ø§Ù…Ø§Øª Ø§Ù„Ù„ÙŠ Ø·Ù„Ø¨ØªÙ‡Ù… ğŸ‘‡"
                    )
            
            intro_msg = generate_accurate_intro_message(quality_filtered_results, location, compound_name)
            # Compute similarity-like scores from chroma distance (not shown in UI)
            similarity_scores = []
            for itm in quality_filtered_results[:10]:
                try:
                    dist = float(itm.get('chroma_score', 0))
                    sim = 1.0 - dist
                    if sim < 0:
                        sim = 0.0
                    if sim > 1:
                        sim = 1.0
                    similarity_scores.append(round(sim, 4))
                except Exception:
                    similarity_scores.append(None)
            
                # Progressive search follow-up message
                if search_round == 1:
                    follow_up_msg = "âœ¨ \"ØªØ­Ø¨ Ø£ÙˆØ±ÙŠÙƒ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ± Ø¹Ù† ÙˆØ­Ø¯Ø© Ù…Ø¹ÙŠÙ†Ø©ØŸ Ø§Ø¨Ø¹ØªÙ„ÙŠ Ø±Ù‚Ù… Ø§Ù„Ù€ID Ø§Ù„Ù„ÙŠ Ø´Ø¯ Ø§Ù†ØªØ¨Ø§Ù‡Ùƒ ğŸ”\nØ£Ùˆ ØªØ­Ø¨ Ø£Ø¹Ø±Ø¶Ù„Ùƒ Ø®ÙŠØ§Ø±Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŸ\""
                elif search_round == 2:
                    follow_up_msg = "âœ¨ \"ØªØ­Ø¨ Ø£ÙˆØ±ÙŠÙƒ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ± Ø¹Ù† ÙˆØ­Ø¯Ø© Ù…Ø¹ÙŠÙ†Ø©ØŸ Ø§Ø¨Ø¹ØªÙ„ÙŠ Ø±Ù‚Ù… Ø§Ù„Ù€ID Ø§Ù„Ù„ÙŠ Ø´Ø¯ Ø§Ù†ØªØ¨Ø§Ù‡Ùƒ ğŸ”\nØ£Ùˆ ØªØ­Ø¨ Ø£Ø¹Ø±Ø¶Ù„Ùƒ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø®ÙŠØ§Ø±Ø§ØªØŸ \""
                else:
                    follow_up_msg = "âœ¨ \"ØªØ­Ø¨ Ø£ÙˆØ±ÙŠÙƒ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ± Ø¹Ù† ÙˆØ­Ø¯Ø© Ù…Ø¹ÙŠÙ†Ø©ØŸ Ø§Ø¨Ø¹ØªÙ„ÙŠ Ø±Ù‚Ù… Ø§Ù„Ù€ID Ø§Ù„Ù„ÙŠ Ø´Ø¯ Ø§Ù†ØªØ¨Ø§Ù‡Ùƒ ğŸ”\nØ£Ùˆ ØªØ­Ø¨ Ø£Ø¹Ø±Ø¶Ù„Ùƒ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø®ÙŠØ§Ø±Ø§ØªØŸ \""
                # Store search parameters in session for progressive search (first round only)
                if search_round == 1:
                    import config
                    session_id_for_storage = arguments.get('session_id', 'default')
                    session_key = f"{session_id_for_storage}_search_history"
                    shown_ids = [str(item.get('id', '')) for item in quality_filtered_results[:10]]
                    
                    # Debug: Print session storage
                    logging.info(f"ğŸ’¾ Storing search history with session_id: {session_id_for_storage}")
                    logging.info(f"ğŸ’¾ Session key: {session_key}")
                    logging.info(f"ğŸ’¾ Shown IDs: {shown_ids[:5]}...")
                    
                    config.client_sessions[session_key] = {
                        "last_search_params": {
                            "location": location,
                            "budget": budget,
                            "property_type": property_type,
                            "bedrooms": bedrooms,
                            "bathrooms": bathrooms,
                            "compound_name": compound_name,
                            "session_id": arguments.get('session_id', 'default')
                        },
                        "search_round": 1,
                        "shown_ids": shown_ids
                    }
                
                return {
                    "source": "chromadb_semantic_search_mmr",
                    "message": intro_msg,
                    "results": formatted_lines,
                    "follow_up": follow_up_msg,
                    "similarity_scores": similarity_scores
                }
            
        except ImportError as e:
            print(f"ğŸš¨ Error importing ChromaDB RAG: {e}")
            return {
                "source": "error",
                "message": "âŒ Ø®Ø·Ø£ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« - ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹.",
                "results": []
            }
        except Exception as e:
            print(f"ğŸš¨ Error in ChromaDB search: {e}")
            return {
                "source": "error",
                "message": "âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« - ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹.",
                "results": []
            }

    except Exception as e:
        print(f"ğŸš¨ Error in property_search: {e}")
        return {"error": str(e)}


def serialize_mysql_result(results):
    """
    Convert MySQL result datetimes or other non-serializable types to JSON-serializable formats.
    """
    for row in results:
        for key, value in row.items():
            if isinstance(value, (datetime.date, datetime.datetime)):
                row[key] = value.strftime('%Y-%m-%d %H:%M:%S')
    return results



from datetime import datetime

def log_conversation_to_db(conversation_id, user_id, message):
    try:
        conversations = load_from_cache("conversations_cache.json")

        # Find existing conversation or create a new one
        convo = next((c for c in conversations if c["conversation_id"] == conversation_id), None)
        now_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        if not convo:
            convo = {
                "conversation_id": conversation_id,
                "user_id": user_id,
                "description": [],
                "created_at": now_str,
                "updated_at": now_str
            }
            conversations.append(convo)
        else:
            convo["updated_at"] = now_str  # Update timestamp

        # Append new message
        convo["description"].append({
            "sender": "Client" if user_id != "bot" else "Bot",
            "message": message,
            "timestamp": now_str
        })

        # Save to cache in background (non-blocking)
        try:
            save_to_cache("conversations_cache.json", conversations)
            append_to_cache("conversations_updates.json", convo)
            logging.info(f"ğŸ“ Updated conversation thread: {conversation_id}")
        except Exception as e:
            logging.warning(f"Cache save failed: {e}")
            
    except Exception as e:
        logging.error(f"Error in log_conversation_to_db: {e}")



def advanced_conversation_summary_from_db(client_id, conversation_id, name="Unknown", property_id="Unknown"):
    """
    Fetch conversation from cache (new structure), summarize it using OpenAI, and return the summary.
    """
    try:
        # 1ï¸âƒ£ Load cached conversations
        conversations = load_from_cache("conversations_cache.json")

        # 2ï¸âƒ£ Find the matching conversation - try multiple lookup strategies
        convo = None
        
        # Strategy 1: Exact match on conversation_id and user_id
        convo = next(
            (c for c in conversations if str(c["conversation_id"]) == str(conversation_id) and str(c["user_id"]) == str(client_id)),
            None
        )
        
        # Strategy 2: If not found, try just conversation_id match
        if not convo:
            convo = next(
                (c for c in conversations if str(c["conversation_id"]) == str(conversation_id)),
                None
            )
        
        # Strategy 3: If still not found, try user_id match
        if not convo:
            convo = next(
                (c for c in conversations if str(c["user_id"]) == str(client_id)),
                None
            )

        if not convo:
            # Debug: Log what we're looking for and what's available
            logging.warning(f"ğŸ” Conversation lookup failed:")
            logging.warning(f"   Looking for: conversation_id='{conversation_id}', client_id='{client_id}'")
            logging.warning(f"   Available conversations: {len(conversations)}")
            if conversations:
                sample_convo = conversations[0]
                logging.warning(f"   Sample conversation: conversation_id='{sample_convo.get('conversation_id')}', user_id='{sample_convo.get('user_id')}'")
            # Return a basic summary when no conversation is found
            return f"Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: Ø§Ù„Ø¹Ù…ÙŠÙ„ {name} (ID: {client_id}) Ø·Ù„Ø¨ Ù…Ø¹Ø§ÙŠÙ†Ø© Ù„Ù„Ø¹Ù‚Ø§Ø± {property_id}. Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…ÙØµÙ„Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©."

        conversation_data = convo.get("description", [])
        if not isinstance(conversation_data, list):
            conversation_data = json.loads(conversation_data)

        # 3ï¸âƒ£ Format conversation for prompt
        formatted_conversation = "\n".join(
            f"{msg['sender']}: {msg['message']}" for msg in conversation_data
        )

        # 4ï¸âƒ£ Create Arabic prompt for summarization with enhanced system instructions
        prompt = f"""
        Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨ÙŠØ¹Ø§Øª Ø¹Ù‚Ø§Ø±ÙŠ Ù…Ø­ØªØ±Ù ÙˆÙ…ØªØ®ØµØµ ÙÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØªÙ‚Ø¯ÙŠÙ… Ù…Ù„Ø®Øµ Ø´Ø§Ù…Ù„ ÙˆÙ…ÙÙŠØ¯ Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª.

        **ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:**
        - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ù…Ø­ØªØ±ÙØ©
        - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ù„Ù…ÙÙŠØ¯Ø© Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª
        - Ø§ÙƒØªØ¨ Ø§Ù„Ù…Ù„Ø®Øµ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù… ÙˆÙˆØ§Ø¶Ø­
        - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù‚Ø§Ø· ÙˆØ§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        - Ø§Ø°ÙƒØ± Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø¨Ø¯Ù‚Ø©

        **Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ø®Øµ:**
        1. **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**
           - Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ø¬Ø¯ÙŠØ¯/Ù…ØªÙƒØ±Ø±)
           - Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… (Ø¹Ø§Ù„Ù/Ù…ØªÙˆØ³Ø·/Ù…Ù†Ø®ÙØ¶)

        2. **Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¹Ù‚Ø§Ø±:**
           - Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (Ø´Ù‚Ø©/ÙÙŠÙ„Ø§/ØªØ¬Ø§Ø±ÙŠ)
           - Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ÙØ¶Ù„
           - Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
           - Ø¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù ÙˆØ§Ù„Ø­Ù…Ø§Ù…Ø§Øª
           - Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

        3. **ØªÙØ¶ÙŠÙ„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:**
           - Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯ Ø§Ù„Ù…ÙØ¶Ù„
           - Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù… (Ø¬Ø§Ù‡Ø²/ØªØ­Øª Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡)
           - Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ù…ÙØ¶Ù„
           - Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„Ø´Ø±Ø§Ø¡ (Ø³ÙƒÙ†/Ø§Ø³ØªØ«Ù…Ø§Ø±)

        4. **Ù†Ù‚Ø§Ø· Ù…Ù‡Ù…Ø©:**
           - Ø£ÙŠ Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª Ø£Ùˆ Ù…Ø®Ø§ÙˆÙ
           - Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙØ¶Ù„Ø© Ù„Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©
           - Ø£ÙŠ Ø·Ù„Ø¨Ø§Øª Ø®Ø§ØµØ©
           - Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„

        5. **Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:**
           - Ù†ÙˆØ¹ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
           - Ø£ÙØ¶Ù„ ÙˆÙ‚Øª Ù„Ù„ØªÙˆØ§ØµÙ„

        **Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:**
        {formatted_conversation}

        **Ø§Ù„Ù…Ù„Ø®Øµ:**
        """

        print(f"ğŸ“‹ Prompt for summarization: {prompt}")

        # 5ï¸âƒ£ Generate summary with Gemini
        try:
            # Configure Gemini with API key
            api_key = os.environ.get('GEMINI_API_KEY')
            if not api_key:
                return "âŒ GEMINI_API_KEY environment variable is not set"
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(variables.GEMINI_MODEL_NAME)
            
            # Create the prompt for Gemini with enhanced system instructions
            gemini_prompt = f"""
            Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨ÙŠØ¹Ø§Øª Ø¹Ù‚Ø§Ø±ÙŠ Ù…Ø­ØªØ±Ù ÙˆÙ…ØªØ®ØµØµ ÙÙŠ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ©. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØªÙ‚Ø¯ÙŠÙ… Ù…Ù„Ø®Øµ Ø´Ø§Ù…Ù„ ÙˆÙ…ÙÙŠØ¯ Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª.

            **ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…:**
            - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ù…Ø­ØªØ±ÙØ©
            - Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙˆØ§Ù„Ù…ÙÙŠØ¯Ø© Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª
            - Ø§ÙƒØªØ¨ Ø§Ù„Ù…Ù„Ø®Øµ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù… ÙˆÙˆØ§Ø¶Ø­
            - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù‚Ø§Ø· ÙˆØ§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
            - Ø§Ø°ÙƒØ± Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø¨Ø¯Ù‚Ø©

            **Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ø®Øµ:**
            1. **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**
               - Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ø¬Ø¯ÙŠØ¯/Ù…ØªÙƒØ±Ø±)
               - Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… (Ø¹Ø§Ù„Ù/Ù…ØªÙˆØ³Ø·/Ù…Ù†Ø®ÙØ¶)

            2. **Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¹Ù‚Ø§Ø±:**
               - Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (Ø´Ù‚Ø©/ÙÙŠÙ„Ø§/ØªØ¬Ø§Ø±ÙŠ)
               - Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ÙØ¶Ù„
               - Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
               - Ø¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù ÙˆØ§Ù„Ø­Ù…Ø§Ù…Ø§Øª
               - Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

            3. **ØªÙØ¶ÙŠÙ„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:**
               - Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯ Ø§Ù„Ù…ÙØ¶Ù„
               - Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù… (Ø¬Ø§Ù‡Ø²/ØªØ­Øª Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡)
               - Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ù…ÙØ¶Ù„
               - Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„Ø´Ø±Ø§Ø¡ (Ø³ÙƒÙ†/Ø§Ø³ØªØ«Ù…Ø§Ø±)

            4. **Ù†Ù‚Ø§Ø· Ù…Ù‡Ù…Ø©:**
               - Ø£ÙŠ Ø§Ø¹ØªØ±Ø§Ø¶Ø§Øª Ø£Ùˆ Ù…Ø®Ø§ÙˆÙ
               - Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù…ÙØ¶Ù„Ø© Ù„Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©
               - Ø£ÙŠ Ø·Ù„Ø¨Ø§Øª Ø®Ø§ØµØ©
               - Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„

            5. **Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:**
               - Ù†ÙˆØ¹ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
               - Ø£ÙØ¶Ù„ ÙˆÙ‚Øª Ù„Ù„ØªÙˆØ§ØµÙ„

            **Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:**
            {formatted_conversation}

            **Ø§Ù„Ù…Ù„Ø®Øµ:**
            """
            
            response = model.generate_content(gemini_prompt)
            summary = response.text.strip()
            print(f"ğŸ“ Generated summary: {summary}")
            return summary
            
        except Exception as e:
            print(f"ğŸš¨ Error generating summary with Gemini: {e}")
            return f"âŒ Ø­ØµÙ„ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {e}"

    except Exception as e:
        print(f"ğŸš¨ Error generating summary: {e}")
        return f"âŒ Ø­ØµÙ„ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {e}"


def classify_query_type_with_llm(user_query):
    """
    ØµÙ†Ù Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¨Ø­Ø« Ø¹Ù† "Ø¥Ø·Ù„Ø§Ù‚ Ø¬Ø¯ÙŠØ¯" Ø£Ùˆ "ÙˆØ­Ø¯Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©" Ø£Ùˆ "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini LLM.
    Returns: 'new_launch', 'existing_unit', or 'unspecified'
    """
    import os
    import variables
    try:
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            # Fallback to direct variables
            api_key = variables.GEMINI_API_KEY
            
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(variables.GEMINI_MODEL_NAME)
        
        # Enhanced prompt with clear definitions and examples
        prompt = f"""
        Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¹Ù‚Ø§Ø±ÙŠ Ø°ÙƒÙŠ. Ù‚Ù… Ø¨ØªØµÙ†ÙŠÙ Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:

        ğŸ”´ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© (Existing Units):
        - ÙˆØ­Ø¯Ø§Øª ØªØ­Øª Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ ØªØ­ØªØ§Ø¬ ØªØ´Ø·ÙŠØ¨
        - ÙˆØ­Ø¯Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø§Ø³ØªÙ„Ø§Ù… ÙˆØ§Ù„Ø³ÙƒÙ† Ø§Ù„ÙÙˆØ±ÙŠ
        - ÙŠÙ…ÙƒÙ† Ù…Ø¹Ø§ÙŠÙ†ØªÙ‡Ø§ Ø´Ø®ØµÙŠØ§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„Ø´Ø±Ø§Ø¡
        - Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ù…Ø­Ø¯Ø¯Ø© ÙˆÙ…Ø¹Ø±ÙˆÙØ©
        - Ù…ØªÙˆÙØ±Ø© ÙÙŠ units.json
        - ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„ÙŠÙ‡Ø§ Ù‚Ø±ÙŠØ¨Ø§Ù‹
        - Ù…ØªÙˆÙØ±Ø© Ø­Ø§Ù„ÙŠØ§Ù‹ Ù„Ù„Ø´Ø±Ø§Ø¡

        ğŸŸ¢ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© (New Launches):
        - ÙˆØ­Ø¯Ø§Øª Ø£Ø¹Ù„Ù† Ø¹Ù†Ù‡Ø§ Ù„Ø£ÙˆÙ„ Ù…Ø±Ø©
        - ÙÙŠ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø­Ø¬Ø² Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ù†Ø§Ø¡
        - Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù‚Ø¯ ØªÙƒÙˆÙ† ØºÙŠØ± Ù…Ø¹Ù„Ù†Ø©
        - ÙŠÙ…ÙƒÙ† Ø§Ù„Ø­Ø¬Ø² Ù…Ø¨ÙƒØ±Ø§Ù‹ Ø¨Ø£Ø³Ø¹Ø§Ø± Ø£Ù‚Ù„
        - Ù…ØªÙˆÙØ±Ø© ÙÙŠ new_launches.json
        - Ù…Ø´Ø§Ø±ÙŠØ¹ Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©
        - Ù„Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø·ÙˆÙŠÙ„
        - Ù„Ù… ØªØ¨Ø¯Ø£ Ø¨Ø¹Ø¯ ÙÙŠ Ø§Ù„Ø¨Ù†Ø§Ø¡

        ğŸ¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙŠØ© ÙˆØ§Ù„Ù‡Ø¯Ù:
        - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ±ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø± Ø£Ùˆ Ù…Ø´Ø§Ø±ÙŠØ¹ Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© â†’ new_launch
        - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ±ÙŠØ¯ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø£Ùˆ Ø§Ù„Ø³ÙƒÙ† Ù‚Ø±ÙŠØ¨Ø§Ù‹ â†’ existing_unit
        - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ±ÙŠØ¯ Ø®ØµÙˆÙ…Ø§Øª Ø£Ùˆ Ø£Ø³Ø¹Ø§Ø± Ø£ÙˆÙ„ÙŠØ© â†’ new_launch
        - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ±ÙŠØ¯ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø£Ùˆ Ø´Ø±Ø§Ø¡ ÙÙˆØ±ÙŠ â†’ existing_unit
        - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ±ÙŠØ¯ Ù…Ø§ Ù‡Ùˆ Ù…ØªØ§Ø­ Ø­Ø§Ù„ÙŠØ§Ù‹ â†’ existing_unit
        - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ±ÙŠØ¯ Ù…Ø§ Ù‡Ùˆ Ù…ØªØ§Ø­ Ù„Ù„Ø­Ø¬Ø² â†’ new_launch

        Ø§Ù„Ø³Ø¤Ø§Ù„: "{user_query}"

        Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ù†ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ§Ù„Ù‡Ø¯Ù Ù…Ù† Ø·Ù„Ø¨Ù‡ØŒ Ø«Ù… ØµÙ†Ù Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±:
        Ø§Ù„ØªØµÙ†ÙŠÙ: [new_launch/existing_unit/unspecified]
        Ø§Ù„Ø³Ø¨Ø¨: [Ø³Ø¨Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ„]
        """
        
        response = model.generate_content(prompt)
        result = response.text.strip()
        
        print(f"ğŸ” Gemini response: {result}")
        
        # Parse the response more intelligently
        lines = result.split('\n')
        classification = "unspecified"
        
        # Look for classification in the response
        for line in lines:
            line = line.strip().lower()
            # Check for new_launch indicators
            if any(term in line for term in ['new_launch', 'Ø¥Ø·Ù„Ø§Ù‚ Ø¬Ø¯ÙŠØ¯', 'Ø¥Ø·Ù„Ø§Ù‚', 'Ø¬Ø¯ÙŠØ¯', 'Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯']):
                classification = "new_launch"
                break
            # Check for existing_unit indicators
            elif any(term in line for term in ['existing_unit', 'ÙˆØ­Ø¯Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©', 'Ù…ÙˆØ¬ÙˆØ¯Ø©', 'Ø¬Ø§Ù‡Ø²', 'Ù…ØªÙˆÙØ±']):
                classification = "existing_unit"
                break
        
        # If still unspecified, try to find the classification in the entire response
        if classification == "unspecified":
            result_lower = result.lower()
            if any(term in result_lower for term in ['new_launch', 'Ø¥Ø·Ù„Ø§Ù‚ Ø¬Ø¯ÙŠØ¯', 'Ø¥Ø·Ù„Ø§Ù‚', 'Ø¬Ø¯ÙŠØ¯', 'Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯']):
                classification = "new_launch"
            elif any(term in result_lower for term in ['existing_unit', 'ÙˆØ­Ø¯Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©', 'Ù…ÙˆØ¬ÙˆØ¯Ø©', 'Ø¬Ø§Ù‡Ø²', 'Ù…ØªÙˆÙØ±']):
                classification = "existing_unit"
        
        # If still unspecified, try to infer from keywords in the query
        if classification == "unspecified":
            query_lower = user_query.lower()
            
            # New launch indicators
            new_launch_keywords = [
                'Ø¥Ø·Ù„Ø§Ù‚ Ø¬Ø¯ÙŠØ¯', 'Ø­Ø¬Ø² Ø£ÙˆÙ„ÙŠ', 'Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯', 'Ø­Ø¬Ø² Ù…Ø¨ÙƒØ±', 
                'Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ù†Ø§Ø¡', 'Ø£Ø³Ø¹Ø§Ø± Ø£ÙˆÙ„ÙŠØ©', 'Ø­Ø¬Ø² Ù…Ø¨ÙƒØ±', 'Ù…Ø´Ø±ÙˆØ¹ Ù‚Ø§Ø¯Ù…',
                'new launch', 'early booking', 'pre-construction', 'off-plan',
                'Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ', 'Ø§Ø³ØªØ«Ù…Ø§Ø±', 'Ø®ØµÙ…', 'Ù…Ø¨ÙƒØ±', 'Ù‚Ø§Ø¯Ù…'
            ]
            
            # Existing unit indicators
            existing_unit_keywords = [
                'Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªÙ„Ø§Ù…', 'Ù…Ø¹Ø§ÙŠÙ†Ø©', 'ØªØ´Ø·ÙŠØ¨', 'Ø³Ø¹Ø± Ù…Ø­Ø¯Ø¯', 
                'Ù…ØªÙˆÙØ± Ø§Ù„Ø¢Ù†', 'ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©', 'Ø¬Ø§Ù‡Ø² Ù„Ù„Ø³ÙƒÙ†',
                'ready for delivery', 'inspection', 'finishing', 'available now',
                'Ø§Ù„Ø¢Ù†', 'Ù‚Ø±ÙŠØ¨Ø§Ù‹', 'ÙÙˆØ±Ø§Ù‹', 'Ø­Ø§Ù„ÙŠØ§Ù‹', 'Ù…ØªÙˆÙØ±'
            ]
            
            # Check for new launch keywords
            if any(keyword in query_lower for keyword in new_launch_keywords):
                classification = "new_launch"
            # Check for existing unit keywords
            elif any(keyword in query_lower for keyword in existing_unit_keywords):
                classification = "existing_unit"
            # Default to existing units for general property searches
            else:
                # If user is asking about general properties without specific new launch intent
                general_property_keywords = ['Ø´Ù‚Ø©', 'ÙÙŠÙ„Ø§', 'ÙƒÙ…Ø¨ÙˆÙ†Ø¯', 'Ø¹Ù‚Ø§Ø±', 'ÙˆØ­Ø¯Ø©', 'Ù…Ø´Ø±ÙˆØ¹']
                if any(keyword in query_lower for keyword in general_property_keywords):
                    classification = "existing_unit"
        
        print(f"ğŸ” Query classification: '{user_query}' â†’ {classification}")
        return classification
        
    except Exception as e:
        print(f"ğŸš¨ Error classifying query type with Gemini: {e}")
        # Fallback to keyword-based classification
        return classify_query_type_by_keywords(user_query)

def classify_query_type_by_keywords(user_query):
    """
    Fallback classification using keyword matching when LLM fails
    """
    query_lower = user_query.lower()
    
    # New launch indicators
    new_launch_keywords = [
        'Ø¥Ø·Ù„Ø§Ù‚ Ø¬Ø¯ÙŠØ¯', 'Ø­Ø¬Ø² Ø£ÙˆÙ„ÙŠ', 'Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯', 'Ø­Ø¬Ø² Ù…Ø¨ÙƒØ±', 
        'Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ù†Ø§Ø¡', 'Ø£Ø³Ø¹Ø§Ø± Ø£ÙˆÙ„ÙŠØ©', 'Ø­Ø¬Ø² Ù…Ø¨ÙƒØ±', 'Ù…Ø´Ø±ÙˆØ¹ Ù‚Ø§Ø¯Ù…',
        'new launch', 'early booking', 'pre-construction', 'off-plan'
    ]
    
    # Existing unit indicators
    existing_unit_keywords = [
        'Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªÙ„Ø§Ù…', 'Ù…Ø¹Ø§ÙŠÙ†Ø©', 'ØªØ´Ø·ÙŠØ¨', 'Ø³Ø¹Ø± Ù…Ø­Ø¯Ø¯', 
        'Ù…ØªÙˆÙØ± Ø§Ù„Ø¢Ù†', 'ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©', 'Ø¬Ø§Ù‡Ø² Ù„Ù„Ø³ÙƒÙ†',
        'ready for delivery', 'inspection', 'finishing', 'available now'
    ]
    
    # Check for new launch keywords
    if any(keyword in query_lower for keyword in new_launch_keywords):
        return "new_launch"
    
    # Check for existing unit keywords
    if any(keyword in query_lower for keyword in existing_unit_keywords):
        return "existing_unit"
    
    # Default to existing units for general property searches
    general_property_keywords = ['Ø´Ù‚Ø©', 'ÙÙŠÙ„Ø§', 'ÙƒÙ…Ø¨ÙˆÙ†Ø¯', 'Ø¹Ù‚Ø§Ø±', 'ÙˆØ­Ø¯Ø©']
    if any(keyword in query_lower for keyword in general_property_keywords):
        return "existing_unit"
    
    return "unspecified"

def classify_insight_intent_llm(user_query: str) -> str:
    """
    Use LLM to classify non-explicit insight questions into:
    - average_price
    - compound_list
    - cheapest_unit
    - other
    """
    try:
        import os
        import google.generativeai as genai
        model_name = variables.GEMINI_MODEL_NAME
        api_key = os.environ.get('GEMINI_API_KEY') or variables.GEMINI_API_KEY
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(model_name)

        prompt = f"""
        ØµÙ†Ù‘Ù Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ù…Ù† Ø§Ù„ÙØ¦Ø§Øª:
        - average_price: Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ³Ø£Ù„ Ø¹Ù† Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ø³Ø¹Ø§Ø±
        - compound_list: Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ³Ø£Ù„ Ø¹Ù† Ø£Ø³Ù…Ø§Ø¡/Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯Ø§Øª
        - cheapest_unit: Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ³Ø£Ù„ Ø¹Ù† Ø§Ù„Ø£Ø±Ø®Øµ
        - other: ØºÙŠØ± Ø°Ù„Ùƒ

        Ø§Ù„Ø³Ø¤Ø§Ù„: "{user_query}"

        Ø£Ø¹Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ù…Ù† Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª: average_price Ø£Ùˆ compound_list Ø£Ùˆ cheapest_unit Ø£Ùˆ other
        """
        resp = model.generate_content(prompt)
        text = (resp.text or '').strip().lower()
        for label in ["average_price", "compound_list", "cheapest_unit", "other"]:
            if label in text:
                return label
        return "other"
    except Exception:
        # Fallback: conservative default
        uq = user_query.lower()
        if "Ù…ØªÙˆØ³Ø·" in uq or "average" in uq:
            return "average_price"
        if "ÙƒÙ…Ø¨ÙˆÙ†Ø¯" in uq or "compound" in uq or "Ø§Ù„ÙƒÙˆÙ…Ø¨ÙˆÙ†Ø¯" in uq:
            return "compound_list"
        if "Ø§Ø±Ø®Øµ" in uq or "Ø§Ù„Ø£Ø±Ø®Øµ" in uq or "Ø§Ù„Ø§Ø±Ø®Øµ" in uq or "cheapest" in uq:
            return "cheapest_unit"
        return "other"

def route_property_search(user_query, search_arguments):
    """
    Route property search to appropriate data source based on LLM classification.
    This function intelligently determines whether to search new launches or existing units.
    
    Args:
        user_query: User's search query
        search_arguments: Dictionary containing search parameters (budget, location, etc.)
    
    Returns:
        Dictionary with search results and source information
    """
    try:
        # First, classify the query type
        query_type = classify_query_type_with_llm(user_query)
        
        print(f"ğŸ” Query '{user_query}' classified as: {query_type}")
        
        # Route based on classification
        if query_type == "new_launch":
            print("ğŸ¯ Routing to new launches search...")
            return search_new_launches(search_arguments)
            
        elif query_type == "existing_unit":
            print("ğŸ¯ Routing to existing units search...")
            return property_search(search_arguments)
            
        else:
            # If unspecified, try to make an educated guess based on context
            print("ğŸ¤” Query type unspecified, making educated guess...")
            
            # Check if user explicitly mentioned new launch concepts
            query_lower = user_query.lower()
            new_launch_indicators = [
                'Ø¥Ø·Ù„Ø§Ù‚ Ø¬Ø¯ÙŠØ¯', 'Ø­Ø¬Ø² Ø£ÙˆÙ„ÙŠ', 'Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø¯ÙŠØ¯', 'Ø­Ø¬Ø² Ù…Ø¨ÙƒØ±',
                'Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ù†Ø§Ø¡', 'Ø£Ø³Ø¹Ø§Ø± Ø£ÙˆÙ„ÙŠØ©', 'Ù…Ø´Ø±ÙˆØ¹ Ù‚Ø§Ø¯Ù…'
            ]
            
            if any(indicator in query_lower for indicator in new_launch_indicators):
                print("ğŸ¯ Detected new launch intent, routing to new launches...")
                return search_new_launches(search_arguments)
            else:
                # Default to existing units for general property searches
                print("ğŸ¯ Defaulting to existing units search...")
                return property_search(search_arguments)
                
    except Exception as e:
        print(f"ğŸš¨ Error in route_property_search: {e}")
        # Fallback to existing units search
        return property_search(search_arguments)

def smart_property_search(user_query, search_arguments):
    """
    Enhanced property search that automatically determines the best search strategy.
    Combines classification with intelligent fallbacks and result merging.
    
    Args:
        user_query: User's search query
        search_arguments: Dictionary containing search parameters
    
    Returns:
        Dictionary with comprehensive search results
    """
    try:
        # Get the primary classification
        primary_type = classify_query_type_with_llm(user_query)
        
        results = {
            "query_type": primary_type,
            "primary_results": None,
            "secondary_results": None,
            "message": "",
            "source": "smart_search"
        }
        
        if primary_type == "new_launch":
            # Search new launches first
            new_launch_results = search_new_launches(search_arguments)
            results["primary_results"] = new_launch_results
            
            # Also search existing units as backup if new launches don't have enough results
            if not new_launch_results.get("results") or len(new_launch_results["results"]) < 3:
                existing_results = property_search(search_arguments)
                results["secondary_results"] = existing_results
                results["message"] = f"âœ… {new_launch_results.get('message', '')} (Ù…Ø¹ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©)"
            else:
                results["message"] = new_launch_results.get("message", "")
                
        elif primary_type == "existing_unit":
            # Search existing units first
            existing_results = property_search(search_arguments)
            results["primary_results"] = existing_results
            
            # Also search new launches as alternative option
            new_launch_results = search_new_launches(search_arguments)
            results["secondary_results"] = new_launch_results
            results["message"] = f"âœ… {existing_results.get('message', '')} (Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª Ù„Ù„Ø¥Ø·Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©)"
            
        else:
            # Try both and let user choose
            existing_results = property_search(search_arguments)
            new_launch_results = search_new_launches(search_arguments)
            
            results["primary_results"] = existing_results
            results["secondary_results"] = new_launch_results
            results["message"] = "ğŸ” Ø¥Ù„ÙŠÙƒ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ù…Ù† ÙƒÙ„Ø§ Ø§Ù„Ù†ÙˆØ¹ÙŠÙ†: Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆØ§Ù„Ø¥Ø·Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"
        
        return results
        
    except Exception as e:
        print(f"ğŸš¨ Error in smart_property_search: {e}")
        return {
            "error": f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {str(e)}",
            "query_type": "error",
            "source": "smart_search"
        }


def insight_search(arguments):
    """
    Answer non-explicit queries (statistics, lists, availability) strictly from ChromaDB data.
    Strategy: Retrieve ALL available data first, then let LLM analyze and filter insights.
    No hallucinations. If zero data, apologize and include WhatsApp link.
    """
    try:
        query_text = arguments.get("query", "")
        location = (arguments.get("location") or "").strip()
        property_type = (arguments.get("property_type") or "").strip()

        # Prefer LLM-based extraction of preferences over rigid keywords
        if not location or not property_type:
            try:
                llm_prefs = extract_client_preferences_llm(
                    user_message=query_text,
                    conversation_history=None,
                    current_preferences=None,
                    conversation_path="available_units"
                )
                if not property_type:
                    property_type = llm_prefs.get("property_type", "").strip()
                if not location:
                    location = llm_prefs.get("location", "").strip()
            except Exception:
                pass

        from chroma_rag_setup import get_rag_instance
        rag = get_rag_instance()

        # Strategy: Retrieve ALL available data first with broad search
        # Then let LLM analyze and filter the results
        
        # Build broader semantic query to get maximum relevant data
        semantic_parts = []
        if property_type:
            semantic_parts.append(property_type)
        if location:
            semantic_parts.append(location)
        semantic_q = " ".join(semantic_parts) if semantic_parts else query_text

        # Retrieve maximum data with minimal filters to get complete dataset
        # Use broader fetch to ensure we don't miss relevant data
        results = rag.search_units(semantic_q, n_results=2000, filters={})
        
        # Also try new launches for completeness
        launches = rag.search_new_launches(semantic_q, n_results=2000, filters={})

        # Combine all data for comprehensive analysis
        all_data = results + launches

        # No data at all => apologize with WhatsApp
        if not all_data:
            return {
                "source": "insight",
                "message": "Ø¢Ø³Ù ÙŠØ§ ÙÙ†Ø¯Ù…ØŒ Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø­Ø§Ù„ÙŠØ§Ù‹. ØªÙ‚Ø¯Ø± ØªØªÙˆØ§ØµÙ„ ÙÙˆØ±Ø§Ù‹ Ù…Ø¹ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø¹Ù„Ù‰ ÙˆØ§ØªØ³Ø§Ø¨: https://wa.me/201000730208",
                "results": []
            }

        # Now filter the retrieved data based on location/property_type if specified
        if location or property_type:
            filtered_results = []
            for item in all_data:
                metadata = item.get('metadata', {})
                doc = item.get('document', '').lower()
                
                # Check if location matches (in document or metadata)
                location_match = True
                if location:
                    loc_lower = location.lower()
                    location_match = (
                        loc_lower in doc or
                        loc_lower in str(metadata.get('location', '')).lower() or
                        loc_lower in str(metadata.get('city', '')).lower()
                    )
                
                # Check if property type matches
                type_match = True
                if property_type:
                    type_lower = property_type.lower()
                    type_match = (
                        type_lower in doc or
                        type_lower in str(metadata.get('property_type', '')).lower()
                    )
                
                if location_match and type_match:
                    filtered_results.append(item)
            
            # Use filtered results for analysis
            results = filtered_results
        else:
            # Use all retrieved data
            results = all_data

        # If no results after filtering, apologize
        if not results:
            return {
                "source": "insight",
                "message": "Ø¢Ø³Ù ÙŠØ§ ÙÙ†Ø¯Ù…ØŒ Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø£Ùˆ Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨. ØªÙ‚Ø¯Ø± ØªØªÙˆØ§ØµÙ„ ÙÙˆØ±Ø§Ù‹ Ù…Ø¹ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø¹Ù„Ù‰ ÙˆØ§ØªØ³Ø§Ø¨: https://wa.me/201000730208",
                "results": []
            }

        # Classify insight intent via LLM
        intent = classify_insight_intent_llm(query_text)

        # 1) Average price query
        if intent == "average_price":
            prices = []
            for r in results:
                meta = r.get('metadata', {})
                price = meta.get('price_value')
                try:
                    if price is not None:
                        prices.append(float(price))
                except Exception:
                    continue
            if not prices:
                return {
                    "source": "insight",
                    "message": "Ø¢Ø³Ù ÙŠØ§ ÙÙ†Ø¯Ù…ØŒ Ù…Ø´ Ù„Ø§Ù‚ÙŠØª Ø£Ø³Ø¹Ø§Ø± ÙƒØ§ÙÙŠØ© Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·. ØªÙ‚Ø¯Ø± ØªØªÙˆØ§ØµÙ„ Ø¹Ù„Ù‰ ÙˆØ§ØªØ³Ø§Ø¨: https://wa.me/201000730208",
                    "results": []
                }
            avg_price = int(sum(prices) / len(prices))
            return {
                "source": "insight",
                "message": f"ğŸ“Š Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ø³Ø¹Ø§Ø±{' Ù„Ù„Ø´Ù‚Ù‚' if 'Ø´Ù‚Ø©' in property_type else ''} ÙÙŠ {location or 'Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©'} Ø­ÙˆØ§Ù„ÙŠ {avg_price:,} Ø¬Ù†ÙŠÙ‡.",
                "results": []
            }

        # 2) Compounds list query
        if intent == "compound_list":
            compounds = []
            seen = set()
            # Collect all compounds from filtered results
            for r in results:
                meta = r.get('metadata', {})
                comp = meta.get('compound_name') or meta.get('compound_name_ar')
                if comp and comp not in seen:
                    seen.add(comp)
                    compounds.append(comp)
            
            if not compounds:
                return {
                    "source": "insight",
                    "message": "Ø¢Ø³Ù ÙŠØ§ ÙÙ†Ø¯Ù…ØŒ Ù…Ø´ Ù„Ø§Ù‚ÙŠ ÙƒÙˆÙ…Ø¨ÙˆÙ†Ø¯Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ØªÙˆØ§ØµÙ„ ÙˆØ§ØªØ³Ø§Ø¨: https://wa.me/201000730208",
                    "results": []
                }
            
            # If data is massive (>15 compounds), provide a sample with note
            if len(compounds) > 15:
                sample_compounds = compounds[:15]
                return {
                    "source": "insight",
                    "message": f"ğŸ˜ï¸ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ÙƒÙˆÙ…Ø¨ÙˆÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ {location or 'Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©'} (Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹: {len(compounds)} ÙƒÙ…Ø¨ÙˆÙ†Ø¯):",
                    "results": sample_compounds + [f"... ÙˆØ¹Ø¯Ø¯ {len(compounds) - 15} ÙƒÙ…Ø¨ÙˆÙ†Ø¯ Ø¥Ø¶Ø§ÙÙŠ. Ù„Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§."]
                }
            else:
                return {
                    "source": "insight",
                    "message": f"ğŸ˜ï¸ Ø§Ù„ÙƒÙˆÙ…Ø¨ÙˆÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ {location or 'Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©'} ({len(compounds)} ÙƒÙ…Ø¨ÙˆÙ†Ø¯):",
                    "results": compounds
                }

        # 3) Cheapest unit query
        if intent == "cheapest_unit":
            cheapest = None
            cheapest_price = None
            for r in results:
                meta = r.get('metadata', {})
                price = meta.get('price_value')
                try:
                    if price is not None:
                        p = float(price)
                        if cheapest_price is None or p < cheapest_price:
                            cheapest_price = p
                            cheapest = r
                except Exception:
                    continue
            if not cheapest:
                return {
                    "source": "insight",
                    "message": "Ø¢Ø³Ù ÙŠØ§ ÙÙ†Ø¯Ù…ØŒ Ù…Ø´ Ù„Ø§Ù‚ÙŠØª ÙˆØ­Ø¯Ø§Øª Ø¨Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ø¶Ø­Ø©. ØªÙ‚Ø¯Ø± ØªØªÙˆØ§ØµÙ„ Ø¹Ù„Ù‰ ÙˆØ§ØªØ³Ø§Ø¨: https://wa.me/201000730208",
                    "results": []
                }
            meta = cheapest.get('metadata', {})
            unit_id = meta.get('unit_id', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
            doc = cheapest.get('document', '')
            name_ar = ""
            if 'Name (AR):' in doc:
                try:
                    name_ar = doc.split('Name (AR):')[1].split('\n')[0].strip()
                except Exception:
                    name_ar = doc.split('\n')[0][:120]
            else:
                name_ar = doc.split('\n')[0][:120]
            price_val = int(cheapest_price) if cheapest_price is not None else 0
            line = f"ID:{unit_id} | {name_ar} | Ø§Ù„Ø³Ø¹Ø±: {price_val:,} EGP"
            return {
                "source": "insight",
                "message": f"âœ… Ø£Ø±Ø®Øµ ÙˆØ­Ø¯Ø© Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ {location or 'Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©'}:",
                "results": [line]
            }

        # Fallback: return sample of items as insight results without fabrication
        # For massive data sets, provide a representative sample
        sample_size = min(10, len(results))
        lines = []
        for i, r in enumerate(results[:sample_size]):
            meta = r.get('metadata', {})
            doc = r.get('document', '')
            unit_id = meta.get('unit_id', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
            name = doc.split('\n')[0][:120] if doc else 'ØºÙŠØ± Ù…ØªÙˆÙØ±'
            price = meta.get('price_value', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
            price_s = f"{int(price):,}" if isinstance(price, (int, float)) else str(price)
            lines.append(f"{i+1}. ID:{unit_id} | {name} | Ø§Ù„Ø³Ø¹Ø±: {price_s}")
        
        # Add note if there are more results
        message = f"ğŸ” Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© (Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹: {len(results)} ÙˆØ­Ø¯Ø©):"
        if len(results) > sample_size:
            lines.append(f"... ÙˆØ¹Ø¯Ø¯ {len(results) - sample_size} ÙˆØ­Ø¯Ø© Ø¥Ø¶Ø§ÙÙŠØ©. Ù„Ù„Ù…Ø²ÙŠØ¯ ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§.")
        
        return {
            "source": "insight",
            "message": message,
            "results": lines
        }

    except Exception as e:
        print(f"ğŸš¨ Error in insight_search: {e}")
        return {
            "error": f"âŒ Ø­ØµÙ„ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±: {str(e)}"
        }

def extract_client_preferences_llm(user_message, conversation_history=None, current_preferences=None, conversation_path=None):
    """
    Intelligently extract client preferences using LLM, maintaining state across conversation turns.
    This function analyzes the user's intent and extracts preferences without relying solely on rigid keywords.
    Enhanced for step-by-step conversation flow.
    
    Args:
        user_message (str): Current user message
        conversation_history (list): List of previous messages in the conversation
        current_preferences (dict): Previously extracted preferences to build upon
        conversation_path (str): Either "new_launches" or "available_units" to determine required fields
    
    Returns:
        dict: Updated preferences with new information and missing_required_fields list
    """
    try:
        import google.generativeai as genai
        from variables import GEMINI_API_KEY
        
        # Set up Gemini
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        # Build context from conversation history and current preferences
        context = ""
        if conversation_history:
            context += "Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n"
            for msg in conversation_history[-5:]:  # Last 5 messages for context
                context += f"- {msg}\n"
            context += "\n"
        
        if current_preferences:
            context += "Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:\n"
            for key, value in current_preferences.items():
                if value and value != 0:
                    context += f"- {key}: {value}\n"
            context += "\n"
        
        # Create intelligent prompt for preference extraction
        path_instructions = ""
        if conversation_path == "new_launches":
            path_instructions = """
            **Ù…Ø³Ø§Ø± Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© - Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**
            - Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø± (Ø¥Ù„Ø²Ø§Ù…ÙŠ)
            - Ø§Ù„Ù…ÙˆÙ‚Ø¹ (Ø¥Ù„Ø²Ø§Ù…ÙŠ)
            - Ø§Ø³Ù… Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            """
        elif conversation_path == "available_units":
            path_instructions = """
            **Ù…Ø³Ø§Ø± Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© - Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:**
            - Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø± (Ø¥Ù„Ø²Ø§Ù…ÙŠ)
            - Ø§Ù„Ù…ÙˆÙ‚Ø¹ (Ø¥Ù„Ø²Ø§Ù…ÙŠ)
            - Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ© (Ø¥Ù„Ø²Ø§Ù…ÙŠ)
            - Ø§Ø³Ù… Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯ Ø§Ù„Ù…ÙØ¶Ù„ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            - Ø¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            - Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            - Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù… (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            """
        
        prompt = f"""
        Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª.
        
        {context}
        
        {path_instructions}
        
        Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠØ©: "{user_message}"
        
        Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ù„Ø©ØŒ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:
        
        1. **Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±**: Ø´Ù‚Ø©ØŒ ÙÙŠÙ„Ø§ØŒ Ø¯ÙˆØ¨Ù„ÙƒØ³ØŒ Ø¨Ù†ØªÙ‡Ø§ÙˆØ³ØŒ ØªØ¬Ø§Ø±ÙŠØŒ Ø¥Ù„Ø®
        2. **Ø§Ù„Ù…ÙˆÙ‚Ø¹**: Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø£Ùˆ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        3. **Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©**: Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ù…ØªØ§Ø­ (Ø¨Ø§Ù„Ø¯ÙˆÙ„Ø§Ø± Ø£Ùˆ Ø§Ù„Ø¬Ù†ÙŠÙ‡)
        4. **Ø¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù**: Ø¹Ø¯Ø¯ ØºØ±Ù Ø§Ù„Ù†ÙˆÙ… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        5. **Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª**: Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        6. **Ø§Ù„Ù…Ø³Ø§Ø­Ø©**: Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (Ù…ØªØ± Ù…Ø±Ø¨Ø¹)
        7. **Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù…**: Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªÙ„Ø§Ù…ØŒ ØªØ­Øª Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡ØŒ Ù‚Ø±ÙŠØ¨Ø§Ù‹
        8. **Ù†ÙˆØ¹ Ø§Ù„Ø¯ÙØ¹**: ÙƒØ§Ø´ØŒ ØªÙ‚Ø³ÙŠØ·ØŒ Ø³Ù†ÙˆØ§Øª Ø§Ù„ØªÙ‚Ø³ÙŠØ·
        9. **Ø§Ù„ØºØ±Ø¶**: Ø³ÙƒÙ†ØŒ Ø§Ø³ØªØ«Ù…Ø§Ø±ØŒ ØªØ£Ø¬ÙŠØ±
        10. **Ù…ÙˆØ§ØµÙØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©**: Ø£ÙŠ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø®Ø§ØµØ©
        11. **Ø§Ø³Ù… Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯**: Ø§Ø³Ù… Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        
        Ø§ÙƒØªØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ:
        Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ø§Ù„Ù…ÙˆÙ‚Ø¹: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ø¹Ø¯Ø¯_Ø§Ù„ØºØ±Ù: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ø¹Ø¯Ø¯_Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ø§Ù„Ù…Ø³Ø§Ø­Ø©: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ù†ÙˆØ¹_Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù…: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ù†ÙˆØ¹_Ø§Ù„Ø¯ÙØ¹: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ø§Ù„ØºØ±Ø¶: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ù…ÙˆØ§ØµÙØ§Øª_Ø¥Ø¶Ø§ÙÙŠØ©: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        Ø§Ø³Ù…_Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯: [Ø§Ù„Ù‚ÙŠÙ…Ø©]
        
        Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø°ÙƒØ± Ù‚ÙŠÙ…Ø©ØŒ Ø§ÙƒØªØ¨ "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
        """
        
        # Get LLM response
        response = model.generate_content(prompt)
        result = response.text
        
        # Parse the response
        preferences = {
            "property_type": "",
            "location": "",
            "budget": 0,
            "bedrooms": 0,
            "bathrooms": 0,
            "apartment_area": "",
            "delivery_type": "",
            "payment_type": "",
            "purpose": "",
            "additional_specs": "",
            "compound_name": ""
        }
        
        # Parse LLM response
        lines = result.strip().split('\n')
        for line in lines:
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip().lower()
                value = value.strip()
                
                if 'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±' in key or 'property_type' in key:
                    preferences["property_type"] = value if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" else ""
                elif 'Ø§Ù„Ù…ÙˆÙ‚Ø¹' in key or 'location' in key:
                    preferences["location"] = value if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" else ""
                elif 'Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©' in key or 'budget' in key:
                    if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯":
                        # Extract numeric value
                        import re
                        budget_match = re.search(r'(\d+(?:,\d{3})*)', value)
                        if budget_match:
                            budget_str = budget_match.group(1).replace(',', '')
                            if 'Ù…Ù„ÙŠÙˆÙ†' in value or 'million' in value:
                                preferences["budget"] = int(budget_str) * 1_000_000
                            elif 'Ø£Ù„Ù' in value or 'thousand' in value:
                                preferences["budget"] = int(budget_str) * 1_000
                            else:
                                preferences["budget"] = int(budget_str)
                elif 'Ø¹Ø¯Ø¯_Ø§Ù„ØºØ±Ù' in key or 'bedrooms' in key:
                    if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯":
                        import re
                        room_match = re.search(r'(\d+)', value)
                        if room_match:
                            preferences["bedrooms"] = int(room_match.group(1))
                elif 'Ø¹Ø¯Ø¯_Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª' in key or 'bathrooms' in key:
                    if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯":
                        import re
                        bath_match = re.search(r'(\d+)', value)
                        if bath_match:
                            preferences["bathrooms"] = int(bath_match.group(1))
                elif 'Ø§Ù„Ù…Ø³Ø§Ø­Ø©' in key or 'area' in key:
                    preferences["apartment_area"] = value if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" else ""
                elif 'Ù†ÙˆØ¹_Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù…' in key or 'delivery' in key:
                    preferences["delivery_type"] = value if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" else ""
                elif 'Ù†ÙˆØ¹_Ø§Ù„Ø¯ÙØ¹' in key or 'payment' in key:
                    preferences["payment_type"] = value if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" else ""
                elif 'Ø§Ù„ØºØ±Ø¶' in key or 'purpose' in key:
                    preferences["purpose"] = value if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" else ""
                elif 'Ù…ÙˆØ§ØµÙØ§Øª_Ø¥Ø¶Ø§ÙÙŠØ©' in key or 'additional' in key:
                    preferences["additional_specs"] = value if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" else ""
                elif 'Ø§Ø³Ù…_Ø§Ù„ÙƒÙ…Ø¨ÙˆÙ†Ø¯' in key or 'compound' in key:
                    preferences["compound_name"] = value if value != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯" else ""
        
        # Merge with existing preferences (new info overrides old)
        if current_preferences:
            for key in preferences:
                if preferences[key] and preferences[key] != 0 and preferences[key] != "":
                    current_preferences[key] = preferences[key]
            final_preferences = current_preferences
        else:
            final_preferences = preferences
        
        # Determine missing required fields based on conversation path
        missing_required_fields = []
        if conversation_path == "new_launches":
            if not final_preferences.get("property_type"):
                missing_required_fields.append("Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±")
            if not final_preferences.get("location"):
                missing_required_fields.append("Ø§Ù„Ù…ÙˆÙ‚Ø¹")
        elif conversation_path == "available_units":
            if not final_preferences.get("property_type"):
                missing_required_fields.append("Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±")
            if not final_preferences.get("location"):
                missing_required_fields.append("Ø§Ù„Ù…ÙˆÙ‚Ø¹")
            if not final_preferences.get("budget") or final_preferences.get("budget") == 0:
                missing_required_fields.append("Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©")
        
        # Add missing fields to the result
        final_preferences["missing_required_fields"] = missing_required_fields
        
        return final_preferences
        
    except Exception as e:
        print(f"ğŸš¨ Error in LLM preference extraction: {e}")
        # Fallback to keyword-based extraction
        return extract_client_preferences(user_message)


def get_conversation_preferences(conversation_id, user_id):
    """
    Retrieve accumulated preferences from conversation history.
    This function builds a comprehensive preference profile from the entire conversation.
    """
    try:
        # Load conversation history with error handling
        conversations = load_from_cache("conversations_cache.json")
        
        if not conversations:
            logging.warning(f"No conversations found in cache for {conversation_id}")
            return {}
        
        # Find the specific conversation
        conversation = next(
            (c for c in conversations if str(c.get("conversation_id")) == str(conversation_id) and str(c.get("user_id")) == str(user_id)),
            None
        )
        
        if not conversation:
            logging.info(f"No conversation found for {conversation_id} and user {user_id}")
            return {}
        
        # Extract all user messages with error handling
        try:
            description = conversation.get("description", [])
            if not isinstance(description, list):
                logging.warning(f"Invalid description format for conversation {conversation_id}")
                return {}
            
            user_messages = [
                msg["message"] for msg in description
                if isinstance(msg, dict) and msg.get("sender") == "Client" and msg.get("message")
            ]
        except Exception as e:
            logging.error(f"Error extracting user messages from conversation {conversation_id}: {e}")
            return {}
        
        if not user_messages:
            logging.info(f"No user messages found in conversation {conversation_id}")
            return {}
        
        # Build comprehensive preferences from all messages
        accumulated_preferences = {}
        
        for i, message in enumerate(user_messages):
            try:
                # Extract preferences from each message
                message_preferences = extract_client_preferences_llm(
                    message, 
                    user_messages[:i], 
                    accumulated_preferences
                )
                
                # Merge preferences (new info overrides old)
                if isinstance(message_preferences, dict):
                    for key, value in message_preferences.items():
                        if value and value != 0 and value != "":
                            accumulated_preferences[key] = value
            except Exception as e:
                logging.warning(f"Error processing message {i} in conversation {conversation_id}: {e}")
                continue
        
        return accumulated_preferences
        
    except Exception as e:
        logging.error(f"ğŸš¨ Error getting conversation preferences for {conversation_id}: {e}")
        return {}

def intelligent_property_search_with_expansion(user_query, search_arguments, chroma_collection, gemini_api_key):
    """
    Intelligent property search that separates semantic and numeric filtering with adaptive expansion policy.
    
    Args:
        user_query (str): The user's natural language query
        search_arguments (dict): Search parameters including numeric filters
        chroma_collection: ChromaDB collection for vector search
        gemini_api_key (str): API key for Gemini embeddings
    
    Returns:
        dict: Search results with metadata about the search strategy used
    """
    try:
        import google.generativeai as genai
        from mmr_search import GeminiEmbedder, mmr
        
        # Set up Gemini
        genai.configure(api_key=gemini_api_key)
        embedder = GeminiEmbedder(gemini_api_key)
        
        # Extract numeric filters from arguments
        numeric_filters = {
            'price_max': search_arguments.get('budget', 0),
            'bedrooms': search_arguments.get('bedrooms'),
            'bathrooms': search_arguments.get('bathrooms'),
            'area_min': search_arguments.get('apartment_area'),
            'installment_years': search_arguments.get('installment_years'),
            'delivery_type': search_arguments.get('delivery_type', '').strip().lower()
        }
        
        # Build semantic query text (exclude numeric values)
        semantic_parts = []
        
        # Property type
        if search_arguments.get('property_type'):
            semantic_parts.append(search_arguments['property_type'])
        
        # Location
        if search_arguments.get('location'):
            semantic_parts.append(search_arguments['location'])
        
        # Compound/developer names (if available)
        if search_arguments.get('compound_name'):
            semantic_parts.append(search_arguments['compound_name'])
        if search_arguments.get('developer_name'):
            semantic_parts.append(search_arguments['developer_name'])
        
        # Delivery readiness
        if numeric_filters['delivery_type']:
            semantic_parts.append(numeric_filters['delivery_type'])
        
        # Purpose (residential, investment, etc.)
        if search_arguments.get('purpose'):
            semantic_parts.append(search_arguments['purpose'])
        
        # Combine semantic parts
        query_text = " ".join(semantic_parts) if semantic_parts else user_query
        
        print(f"ğŸ” Semantic Query: {query_text}")
        print(f"ğŸ’° Numeric Filters: {numeric_filters}")
        
        # Initialize search strategy
        search_strategy = {
            'filters_applied': [],
            'expansion_steps': [],
            'final_results_count': 0
        }
        
        # Step 1: Start with only price filter
        current_filters = {}
        if numeric_filters['price_max'] > 0:
            current_filters['price_max'] = numeric_filters['price_max']
            search_strategy['filters_applied'].append('price_max')
        
        # Perform initial semantic search with minimal filters
        results = _perform_semantic_search(
            query_text, 
            chroma_collection, 
            embedder, 
            current_filters,
            fetch_k=1000
        )
        
        print(f"ğŸ“Š Initial results: {len(results)}")
        
        # Step 2: Adaptive filtering based on result count
        target_results = 20  # Target number of results
        min_results = 5      # Minimum acceptable results
        
        if len(results) > target_results:
            # Too many results - add filters progressively
            results = _apply_progressive_filtering(
                results, numeric_filters, search_strategy, target_results
            )
        elif len(results) < min_results:
            # Too few results - apply expansion policy
            results = _apply_expansion_policy(
                query_text, chroma_collection, embedder, 
                numeric_filters, search_strategy, min_results
            )
        
        search_strategy['final_results_count'] = len(results)
        
        return {
            'results': results,
            'search_strategy': search_strategy,
            'semantic_query': query_text,
            'numeric_filters': numeric_filters
        }
        
    except Exception as e:
        print(f"ğŸš¨ Error in intelligent property search: {e}")
        # Fallback to basic search
        return {
            'results': [],
            'error': str(e),
            'fallback': True
        }


def _perform_semantic_search(query_text, chroma_collection, embedder, filters, fetch_k=100):
    """
    Perform semantic search with given filters
    """
    try:
        from mmr_search import mmr  # Import mmr here to ensure it's available
        
        # Build where clause for ChromaDB
        where_clause = {}
        
        if 'price_max' in filters:
            tolerance = 0.15  # 15% tolerance for price
            min_price = filters['price_max'] * (1 - tolerance)
            where_clause['price_value'] = {'$lte': filters['price_max']}
        
        # Query ChromaDB
        chroma_results = chroma_collection.query(
            query_texts=[query_text],
            n_results=fetch_k,
            where=where_clause if where_clause else None
        )
        
        docs = chroma_results['documents'][0]
        metadatas = chroma_results['metadatas'][0]
        embeddings = chroma_results['embeddings'][0] if 'embeddings' in chroma_results else None
        
        if embeddings is None:
            embeddings = embedder.embed_many(docs)
        
        # Apply MMR for diversity
        query_embedding = embedder.embed(query_text)
        mmr_indices = mmr(query_embedding, embeddings, k=min(len(docs), 50), lambda_param=0.7)
        
        results = []
        for i in mmr_indices:
            results.append({
                'document': docs[i],
                'metadata': metadatas[i],
                'score': _cosine_similarity(query_embedding, embeddings[i]),
                'source': 'semantic_search'
            })
        
        return results
        
    except Exception as e:
        print(f"ğŸš¨ Error in semantic search: {e}")
        return []


def _apply_progressive_filtering(results, numeric_filters, search_strategy, target_results):
    """
    Apply filters progressively to reduce results to target count
    """
    print(f"ğŸ” Applying progressive filtering to reduce {len(results)} results to ~{target_results}")
    
    # Filter 1: Bedrooms (if specified)
    if numeric_filters['bedrooms'] and len(results) > target_results:
        filtered_results = []
        for result in results:
            result_bedrooms = result['metadata'].get('bedrooms', 0)
            if result_bedrooms == numeric_filters['bedrooms']:
                filtered_results.append(result)
        
        if len(filtered_results) >= target_results * 0.5:  # Keep if we still have enough results
            results = filtered_results
            search_strategy['filters_applied'].append('bedrooms')
            print(f"âœ… Applied bedrooms filter: {len(results)} results")
    
    # Filter 2: Area (if specified)
    if numeric_filters['area_min'] and len(results) > target_results:
        area_tolerance = 30  # Â±30 sqm tolerance
        filtered_results = []
        for result in results:
            result_area = result['metadata'].get('area', 0)
            if abs(result_area - numeric_filters['area_min']) <= area_tolerance:
                filtered_results.append(result)
        
        if len(filtered_results) >= target_results * 0.5:
            results = filtered_results
            search_strategy['filters_applied'].append('area')
            print(f"âœ… Applied area filter: {len(results)} results")
    
    # Filter 3: Bathrooms (if specified)
    if numeric_filters['bathrooms'] and len(results) > target_results:
        filtered_results = []
        for result in results:
            result_bathrooms = result['metadata'].get('bathrooms', 0)
            if result_bathrooms == numeric_filters['bathrooms']:
                filtered_results.append(result)
        
        if len(filtered_results) >= target_results * 0.5:
            results = filtered_results
            search_strategy['filters_applied'].append('bathrooms')
            print(f"âœ… Applied bathrooms filter: {len(results)} results")
    
    return results


def _apply_expansion_policy(query_text, chroma_collection, embedder, numeric_filters, search_strategy, min_results):
    """
    Apply expansion policy when too few results are found
    """
    print(f"ğŸ” Applying expansion policy to increase results above {min_results}")
    
    # Expansion 1: Relax area constraints
    if numeric_filters['area_min']:
        area_tolerance = 50  # Increase to Â±50 sqm
        search_strategy['expansion_steps'].append(f'area_tolerance_increased_to_{area_tolerance}')
        print(f"ğŸ“ Increased area tolerance to Â±{area_tolerance} sqm")
    
    # Expansion 2: Relax bedroom constraints
    if numeric_filters['bedrooms']:
        bedroom_range = f"{max(1, numeric_filters['bedrooms']-1)}-{numeric_filters['bedrooms']+1}"
        search_strategy['expansion_steps'].append(f'bedroom_range_expanded_to_{bedroom_range}')
        print(f"ğŸ›ï¸ Expanded bedroom range to {bedroom_range}")
    
    # Expansion 3: Increase price tolerance
    if numeric_filters['price_max']:
        price_tolerance = 0.25  # Increase to 25%
        search_strategy['expansion_steps'].append(f'price_tolerance_increased_to_{price_tolerance*100}%')
        print(f"ğŸ’° Increased price tolerance to {price_tolerance*100}%")
    
    # Re-run search with relaxed constraints
    relaxed_filters = numeric_filters.copy()
    if numeric_filters['price_max']:
        relaxed_filters['price_max'] = int(numeric_filters['price_max'] * 1.25)
    
    results = _perform_semantic_search(
        query_text, 
        chroma_collection, 
        embedder, 
        relaxed_filters,
        fetch_k=200  # Increase fetch size
    )
    
    print(f"ğŸ“Š Results after expansion: {len(results)}")
    return results


def _cosine_similarity(vec1, vec2):
    """Calculate cosine similarity between two vectors"""
    import numpy as np
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:
        return 0.0
    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))


def get_more_units(arguments):
    """
    Get more units with increased price tolerance for progressive search.
    This is only for existing units, not new launches.
    """
    try:
        # Get the session ID to retrieve previous search parameters
        session_id = arguments.get("session_id", "")
        
        if not session_id:
            return {
                "source": "error",
                "message": "âŒ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©",
                "results": []
            }
        
        # Try to get previous search parameters from session
        import config
        session_key = f"{session_id}_search_history"
        session_data = config.client_sessions.get(session_key)
        
        # Debug: Print available session keys
        logging.info(f"ğŸ” Looking for session key: {session_key}")
        logging.info(f"ğŸ” Available session keys: {list(config.client_sessions.keys())}")
        
        if not session_data:
            # Try alternative session key formats
            # 1. Look for any key containing parts of the session_id
            session_parts = session_id.replace("session_", "").split("_")
            alternative_keys = []
            
            for key in config.client_sessions.keys():
                if "search_history" in key:
                    # Check if any part of the session matches
                    key_parts = key.replace("session_", "").split("_")
                    if any(part in key_parts for part in session_parts if part and len(part) > 4):
                        alternative_keys.append(key)
            
            # 2. If no partial match, try the most recent search_history key
            if not alternative_keys:
                search_history_keys = [key for key in config.client_sessions.keys() if "search_history" in key]
                if search_history_keys:
                    # Sort by key (assuming newer sessions have later timestamps)
                    alternative_keys = sorted(search_history_keys, reverse=True)[:1]
            
            if alternative_keys:
                session_key = alternative_keys[0]
                session_data = config.client_sessions.get(session_key)
                logging.info(f"ğŸ” Found alternative session key: {session_key}")
                logging.info(f"ğŸ” Original session_id: {session_id}")
                logging.info(f"ğŸ” All search_history keys: {[k for k in config.client_sessions.keys() if 'search_history' in k]}")
            
        if not session_data:
            return {
                "source": "error", 
                "message": f"âŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©. Session ID: {session_id}. ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¨Ø­Ø« Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©.",
                "results": []
            }
        
        # Get previous search parameters and round
        previous_params = session_data.get("last_search_params", {})
        current_round = session_data.get("search_round", 1) + 1
        shown_ids = session_data.get("shown_ids", [])
        
        if current_round > 3:
            return {
                "source": "max_rounds_reached",
                "message": "ØªÙ… Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ø¨Ù…Ø®ØªÙ„Ù Ù…Ø³ØªÙˆÙŠØ§Øª Ù…Ø±ÙˆÙ†Ø© Ø§Ù„Ø³Ø¹Ø±. ØªØ­Ø¨ ØªØ¨Ø¯Ø£ Ø¨Ø­Ø« Ø¬Ø¯ÙŠØ¯ Ø¨Ù…Ø¹Ø§ÙŠÙŠØ± Ù…Ø®ØªÙ„ÙØ©ØŸ",
                "results": []
            }
        
        # Set price tolerance based on round
        # Round 1: 10%, Round 2: 20%, Round 3: 25%
        tolerance_map = {2: 20, 3: 25}
        price_tolerance = tolerance_map.get(current_round, 25)
        
        # Update search parameters for progressive search
        new_params = previous_params.copy()
        new_params.update({
            "price_tolerance": price_tolerance,
            "excluded_ids": shown_ids,
            "search_round": current_round
        })
        
        # Call property_search with updated parameters
        result = property_search(new_params)
        
        # Update session with new results
        if result.get("results"):
            # Handle both string and dict formats for results
            new_ids = []
            for item in result.get("results", []):
                if isinstance(item, dict):
                    new_ids.append(str(item.get('id', '')))
                elif isinstance(item, str):
                    import re
                    id_match = re.search(r'ID:(\d+)', item)
                    if id_match:
                        new_ids.append(id_match.group(1))
            
            new_shown_ids = shown_ids + new_ids
            session_data.update({
                "search_round": current_round,
                "shown_ids": new_shown_ids,
                "last_search_params": previous_params  # Keep original params
            })
            config.client_sessions[f"{session_id}_search_history"] = session_data
        
        return result
        
    except Exception as e:
        logging.error(f"Error in get_more_units: {e}")
        return {
            "source": "error",
            "message": f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ­Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©: {str(e)}",
            "results": []
        }

def get_unit_details(arguments):
    """
    Get detailed information about a specific unit by ID.
    Works for both regular properties and new launches.
    """
    try:
        unit_id = arguments.get("unit_id")
        if not unit_id:
            return {
                "error": "âŒ Ø±Ù‚Ù… Ø§Ù„ÙˆØ­Ø¯Ø© Ù…Ø·Ù„ÙˆØ¨ Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙØ§ØµÙŠÙ„."
            }
        
        # First, try to find in new launches (since this is more likely for new launch IDs)
        try:
            from chroma_rag_setup import get_rag_instance
            rag = get_rag_instance()
            nl_results = rag.new_launches_collection.query(
                query_texts=["launch details"],
                n_results=1,
                where={"launch_id": str(unit_id)},
                include=["metadatas", "documents"]
            )
            if nl_results and nl_results.get('documents') and nl_results['documents'][0]:
                doc = nl_results['documents'][0][0]
                meta = nl_results['metadatas'][0][0] if nl_results.get('metadatas') and nl_results['metadatas'][0] else {}
                
                # Extract required fields from embedded document
                def extract_between(label: str, text: str) -> str:
                    if label in text:
                        try:
                            return text.split(label)[1].split('\n')[0].strip()
                        except Exception:
                            return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                    return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                
                # Extract all relevant fields for new launches
                property_type_name = extract_between('Property Type:', doc)
                desc_ar = extract_between('Description (AR):', doc)
                city_name = extract_between('City:', doc)
                compound_name = extract_between('Compound (AR):', doc)
                name_ar = extract_between('Name (AR):', doc)
                price = meta.get('price_value', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                bedrooms = meta.get('bedrooms', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                bathrooms = meta.get('bathrooms', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                area = meta.get('apartment_area', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                delivery_in = meta.get('delivery_in', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                installment_years = meta.get('installment_years', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                new_image = meta.get('new_image', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                
                # Format price if available
                try:
                    if price and price != 'ØºÙŠØ± Ù…ØªÙˆÙØ±':
                        price_val = f"{int(float(price)):,}"
                    else:
                        price_val = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                except:
                    price_val = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                
                formatted_details = f"""
ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙˆØ­Ø¯Ø©:
ğŸ“ Ø§Ø³Ù… Ø§Ù„ÙˆØ­Ø¯Ø©: {name_ar}
ğŸ“ Ø§Ù„ÙˆØµÙ: {desc_ar}

ğŸ“Œ Ø§Ù„ÙƒÙˆÙ…Ø¨Ø§ÙˆÙ†Ø¯: {compound_name}
ğŸ·ï¸ Ø§Ù„Ù†ÙˆØ¹: {property_type_name}
ğŸŒ† Ø§Ù„Ù…ÙˆÙ‚Ø¹: {city_name}

ğŸ–¼ï¸ **ØµÙˆØ±Ø© Ø§Ù„Ø¹Ù‚Ø§Ø±:** {new_image}


â„¹ï¸ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù‡Ø§Ù…Ø©

âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø²ÙŠØ§Ø±Ø© Ù…ÙŠØ¯Ø§Ù†ÙŠØ© Ø­Ø§Ù„ÙŠÙ‹Ø§ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹.

â„¹ï¸ ØªÙØ§ØµÙŠÙ„ Ù…Ø«Ù„ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±ØŒ Ø§Ù„Ù…Ø³Ø§Ø­Ø§ØªØŒ ÙˆØ£Ù†Ø¸Ù…Ø© Ø§Ù„Ø¯ÙØ¹ ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ø§Ù„Ø¢Ù†.

ğŸ¯ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©

ØªØ­Ø¨ Ø£Ø­Ø¬Ø²Ù„Ùƒ Zoom Meeting Ù…Ø¹ Ø³ÙŠÙ„Ø² Ø§Ù„Ù…Ø·ÙˆØ± Ø¹Ø´Ø§Ù† ØªØ¹Ø±Ù ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ± Ø¹Ù† Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…ØªØ§Ø­Ø©ØŸ
"""
                return {
                    "unit_type": "new_launch",
                    "message": formatted_details,
                    "details": {
                        "id": unit_id,
                        "property_type_name": property_type_name,
                        "desc_ar": desc_ar,
                        "city_name": city_name,
                        "new_image": new_image,
                        "name_ar": name_ar,
                        "compound_name": compound_name,
                        "price": price_val,
                        "bedrooms": bedrooms,
                        "bathrooms": bathrooms,
                        "area": area,
                        "delivery_in": delivery_in,
                        "installment_years": installment_years
                    }
                }
        except Exception as e:
            print(f"âš ï¸ Error loading new launch from Chroma: {e}")
        
        # Fallback: try to find in regular units cache
        try:
            units = load_from_cache("units.json")
            for unit in units:
                if str(unit.get("id")) == str(unit_id):
                    # Build values with fallbacks
                    def clean(val):
                        if val is None or str(val).strip() == "" or str(val).strip().lower() == "none":
                            return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                        return str(val)

                    name_ar_val = clean(unit.get("name_ar"))
                    desc_ar_val = clean(unit.get("desc_ar"))
                    compound_ar_val = clean(unit.get("compound_name_ar") or unit.get("compound_name"))
                    area_val = clean(unit.get("apartment_area") or unit.get("size"))

                    price_raw = unit.get("price")
                    try:
                        if price_raw is None:
                            price_val = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                        else:
                            # normalize to int and format with commas
                            pr = str(price_raw).replace(",", "").strip()
                            pr_num = int(float(pr))
                            price_val = f"{pr_num:,}"
                    except Exception:
                        price_val = clean(price_raw)

                    bedrooms_val = clean(unit.get("Bedrooms"))
                    bathrooms_val = clean(unit.get("Bathrooms"))
                    delivery_in_val = clean(unit.get("delivery_in"))
                    installment_years_val = clean(unit.get("installment_years"))
                    address_val = clean(unit.get("address"))
                    image_val = clean(unit.get("new_image") or unit.get("image_url"))

                    # Format the response exactly as requested
                    formatted_details = f"""
ğŸ“ **Ø§Ø³Ù… Ø§Ù„ÙˆØ­Ø¯Ø©:** {name_ar_val}  
ğŸ“ **Ø§Ù„ÙˆØµÙ:** {desc_ar_val}  

ğŸ“Œ **Ø§Ù„ÙƒÙˆÙ…Ø¨Ø§ÙˆÙ†Ø¯:** {compound_ar_val}  
ğŸ“ **Ø§Ù„Ù…Ø³Ø§Ø­Ø©:** {area_val} Ù…Â²  
ğŸ’° **Ø§Ù„Ø³Ø¹Ø±:** {price_val} Ø¬Ù†ÙŠÙ‡  

ğŸ› **Ø¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù:** {bedrooms_val}  
ğŸš½ **Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª:** {bathrooms_val}  

ğŸšš **Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù… Ø®Ù„Ø§Ù„:** {delivery_in_val} Ø³Ù†Ø©  
ğŸ’³ **ØªÙ‚Ø³ÙŠØ· Ø­ØªÙ‰:** {installment_years_val} Ø³Ù†ÙŠÙ†  

ğŸ–¼ï¸ **ØµÙˆØ±Ø© Ø§Ù„Ø¹Ù‚Ø§Ø±:** {image_val}

ØªØ­Ø¨ Ø§Ø­Ø¬Ø²Ù„Ùƒ zoom meeting Ù…Ø¹ Ø§Ù„Ø³ÙŠÙ„Ø² Ø§Ù„Ù…Ø·ÙˆØ± Ø¹Ø´Ø§Ù† ØªØ¹Ø±Ù ØªÙØ§ØµÙŠÙ„ Ø§ÙƒØªØ± Ø§Ùˆ Ø¹Ø±ÙˆØ¶ ÙˆÙ„Ø§ Ø§Ø­Ø¬Ø²Ù„Ùƒ Ø²ÙŠØ§Ø±Ø© on site Ù„Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©
"""
                    return {
                        "unit_type": "regular_property",
                        "message": formatted_details,
                        "details": {
                            "id": unit.get("id"),
                            "name_ar": name_ar_val,
                            "name_en": unit.get("name_en", "ØºÙŠØ± Ù…ØªÙˆÙØ±"),
                            "compound_name_ar": compound_ar_val,
                            "price": price_val,
                            "bedrooms": bedrooms_val,
                            "bathrooms": bathrooms_val,
                            "delivery_in": delivery_in_val,
                            "installment_years": installment_years_val,
                            "location": unit.get("location", "ØºÙŠØ± Ù…ØªÙˆÙØ±"),
                            "new_image": image_val,
                            "desc_ar": desc_ar_val,
                            "address": address_val,
                            "apartment_area": area_val
                        }
                    }
        except Exception as e:
            print(f"âš ï¸ Error loading from units cache: {e}")
        
        # Fallback: try to query Chroma by unit_id
        try:
            from chroma_rag_setup import RealEstateRAG
            rag = RealEstateRAG()
            # Query with where clause to match unit_id
            results = rag.units_collection.query(
                query_texts=["unit details"],
                n_results=1,
                where={"unit_id": str(unit_id)},
                include=["metadatas", "documents", "distances"]
            )
            if results and results.get('documents') and results['documents'][0]:
                doc = results['documents'][0][0]
                meta = results['metadatas'][0][0] if results.get('metadatas') and results['metadatas'][0] else {}
                # Try to parse fields from doc text
                name_ar = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                if 'Name (AR):' in doc:
                    name_ar = doc.split('Name (AR):')[1].split('\n')[0].strip()
                compound_name = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                if 'Compound (AR):' in doc:
                    compound_name = doc.split('Compound (AR):')[1].split('\n')[0].strip()
                price = meta.get('price_value', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                bedrooms = meta.get('bedrooms', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                bathrooms = meta.get('bathrooms', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                area = meta.get('apartment_area', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                image_url = meta.get('new_image', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                
                # Extract description and address from document
                desc_ar = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                address = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                compound_ar = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                if 'Description (AR):' in doc:
                    try:
                        desc_ar = doc.split('Description (AR):')[1].split('\n')[0].strip()
                    except:
                        desc_ar = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                if 'Compound (AR):' in doc:
                    try:
                        compound_ar = doc.split('Compound (AR):')[1].split('\n')[0].strip()
                    except:
                        compound_ar = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                if 'Address:' in doc:
                    try:
                        address = doc.split('Address:')[1].split('\n')[0].strip()
                    except:
                        address = "ØºÙŠØ± Ù…ØªÙˆÙØ±"

                # Build values with fallbacks
                def clean2(val):
                    if val is None or str(val).strip() == "" or str(val).strip().lower() == "none":
                        return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                    return str(val)

                price_meta = meta.get('price_value')
                try:
                    if price_meta is None:
                        price_val = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                    else:
                        price_val = f"{int(float(price_meta)):,}"
                except Exception:
                    price_val = clean2(price_meta)

                name_ar_val = clean2(name_ar)
                area_val = clean2(meta.get('apartment_area'))
                bedrooms_val = clean2(meta.get('bedrooms'))
                bathrooms_val = clean2(meta.get('bathrooms'))
                delivery_in_val = clean2(meta.get('delivery_in'))
                installment_years_val = clean2(meta.get('installment_years'))
                address_val = clean2(address)
                image_val = clean2(meta.get('new_image'))
                compound_ar_val = clean2(compound_ar)
                desc_ar_val = clean2(desc_ar)

                # Format the response exactly as requested
                formatted_details = f"""
ğŸ“ **Ø§Ø³Ù… Ø§Ù„ÙˆØ­Ø¯Ø©:** {name_ar_val}  
ğŸ“ **Ø§Ù„ÙˆØµÙ:** {desc_ar_val}  

ğŸ“Œ **Ø§Ù„ÙƒÙˆÙ…Ø¨Ø§ÙˆÙ†Ø¯:** {compound_ar_val}  
ğŸ“ **Ø§Ù„Ù…Ø³Ø§Ø­Ø©:** {area_val} Ù…Â²  
ğŸ’° **Ø§Ù„Ø³Ø¹Ø±:** {price_val} Ø¬Ù†ÙŠÙ‡  

ğŸ› **Ø¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù:** {bedrooms_val}  
ğŸš½ **Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª:** {bathrooms_val}  

ğŸšš **Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù… Ø®Ù„Ø§Ù„:** {delivery_in_val} Ø³Ù†Ø©  
ğŸ’³ **ØªÙ‚Ø³ÙŠØ· Ø­ØªÙ‰:** {installment_years_val} Ø³Ù†Ø©  

ğŸ–¼ï¸ **ØµÙˆØ±Ø© Ø§Ù„Ø¹Ù‚Ø§Ø±:** {image_val}

ØªØ­Ø¨ Ø§Ø­Ø¬Ø²Ù„Ùƒ zoom meeting Ù…Ø¹ Ø§Ù„Ø³ÙŠÙ„Ø² Ø§Ù„Ù…Ø·ÙˆØ± Ø¹Ø´Ø§Ù† ØªØ¹Ø±Ù ØªÙØ§ØµÙŠÙ„ Ø§ÙƒØªØ± Ø§Ùˆ Ø¹Ø±ÙˆØ¶ ÙˆÙ„Ø§ Ø§Ø­Ø¬Ø²Ù„Ùƒ Ø²ÙŠØ§Ø±Ø© on site Ù„Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©
"""
                return {
                    "unit_type": "regular_property",
                    "message": formatted_details,
                    "details": {
                        "id": unit_id,
                        "name_ar": name_ar_val,
                        "name_en": doc.split('\n')[0][:120] if doc else "ØºÙŠØ± Ù…ØªÙˆÙØ±",
                        "compound_name_ar": compound_ar_val,
                        "price": price_val,
                        "bedrooms": bedrooms_val,
                        "bathrooms": bathrooms_val,
                        "delivery_in": delivery_in_val,
                        "installment_years": installment_years_val,
                        "location": meta.get('location', 'ØºÙŠØ± Ù…ØªÙˆÙØ±'),
                        "new_image": image_val,
                        "desc_ar": desc_ar_val,
                        "address": address_val,
                        "apartment_area": area_val
                    }
                }
        except Exception as e:
            print(f"âš ï¸ Error fetching from Chroma for unit {unit_id}: {e}")
        
        # Try to find in new launches
        try:
            # Prefer ChromaDB lookup for new launches to extract from embeddings/metadata
            from chroma_rag_setup import RealEstateRAG
            rag = RealEstateRAG()
            nl_results = rag.new_launches_collection.query(
                query_texts=["launch details"],
                n_results=1,
                where={"launch_id": str(unit_id)},
                include=["metadatas", "documents"]
            )
            if nl_results and nl_results.get('documents') and nl_results['documents'][0]:
                doc = nl_results['documents'][0][0]
                meta = nl_results['metadatas'][0][0] if nl_results.get('metadatas') and nl_results['metadatas'][0] else {}
                
                # Extract required fields from embedded document
                def extract_between(label: str, text: str) -> str:
                    if label in text:
                        try:
                            return text.split(label)[1].split('\n')[0].strip()
                        except Exception:
                            return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                    return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                
                # Extract all relevant fields for new launches
                property_type_name = extract_between('Property Type:', doc)
                desc_ar = extract_between('Description (AR):', doc)
                city_name = extract_between('City:', doc)
                compound_name = extract_between('Compound (AR):', doc)
                name_ar = extract_between('Name (AR):', doc)
                price = meta.get('price_value', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                bedrooms = meta.get('bedrooms', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                bathrooms = meta.get('bathrooms', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                area = meta.get('apartment_area', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                delivery_in = meta.get('delivery_in', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                installment_years = meta.get('installment_years', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                new_image = meta.get('new_image', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')
                
                # Format price if available
                try:
                    if price and price != 'ØºÙŠØ± Ù…ØªÙˆÙØ±':
                        price_val = f"{int(float(price)):,}"
                    else:
                        price_val = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                except:
                    price_val = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                
                formatted_details = f"""
ğŸ“ **Ø§Ø³Ù… Ø§Ù„ÙˆØ­Ø¯Ø©:** {name_ar}
ğŸ“ **Ø§Ù„ÙˆØµÙ:** {desc_ar}

ğŸ“Œ **Ø§Ù„ÙƒÙˆÙ…Ø¨Ø§ÙˆÙ†Ø¯:** {compound_name}
ğŸ“ **Ø§Ù„Ù…Ø³Ø§Ø­Ø©:** {area} Ù…Â²
ğŸ’° **Ø§Ù„Ø³Ø¹Ø±:** {price_val} Ø¬Ù†ÙŠÙ‡

ğŸ› **Ø¹Ø¯Ø¯ Ø§Ù„ØºØ±Ù:** {bedrooms}
ğŸš½ **Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª:** {bathrooms}

ğŸšš **Ø§Ù„Ø§Ø³ØªÙ„Ø§Ù… Ø®Ù„Ø§Ù„:** {delivery_in} Ø³Ù†Ø©
ğŸ’³ **ØªÙ‚Ø³ÙŠØ· Ø­ØªÙ‰:** {installment_years} Ø³Ù†Ø©

ğŸ–¼ï¸ **ØµÙˆØ±Ø© Ø§Ù„Ø¹Ù‚Ø§Ø±:** {new_image}

ØªØ­Ø¨ Ø§Ø­Ø¬Ø²Ù„Ùƒ zoom meeting Ù…Ø¹ Ø§Ù„Ø³ÙŠÙ„Ø² Ø§Ù„Ù…Ø·ÙˆØ± Ø¹Ø´Ø§Ù† ØªØ¹Ø±Ù ØªÙØ§ØµÙŠÙ„ Ø§ÙƒØªØ± Ø§Ùˆ Ø¹Ø±ÙˆØ¶ ÙˆÙ„Ø§ Ø§Ø­Ø¬Ø²Ù„Ùƒ Ø²ÙŠØ§Ø±Ø© on site Ù„Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©
"""
                return {
                    "unit_type": "new_launch",
                    "message": formatted_details,
                    "details": {
                        "id": unit_id,
                        "property_type_name": property_type_name,
                        "desc_ar": desc_ar,
                        "city_name": city_name,
                        "new_image": new_image,
                        "name_ar": name_ar,
                        "compound_name": compound_name,
                        "price": price_val,
                        "bedrooms": bedrooms,
                        "bathrooms": bathrooms,
                        "area": area,
                        "delivery_in": delivery_in,
                        "installment_years": installment_years
                    }
                }
        except Exception as e:
            print(f"âš ï¸ Error loading new launch from Chroma: {e}")
        
        # Fallback to cache new_launches if Chroma lookup fails
        try:
            new_launches = load_from_cache("new_launches.json")
            for launch in new_launches:
                if str(launch.get("id")) == str(unit_id):
                    # Minimal fields per request; best-effort from cache
                    property_type_name = launch.get("property_type_name", "ØºÙŠØ± Ù…ØªÙˆÙØ±")
                    desc_ar = launch.get("desc_ar", "ØºÙŠØ± Ù…ØªÙˆÙØ±")
                    city_name = launch.get("city", launch.get("city_name", "ØºÙŠØ± Ù…ØªÙˆÙØ±"))
                    new_image = launch.get("new_image", launch.get("image_url", "ØºÙŠØ± Ù…ØªÙˆÙØ±"))
                    formatted_details = f"""
Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±: {property_type_name}
Ø§Ù„ÙˆØµÙ: {desc_ar}
Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©: {city_name}
Ø§Ù„ØµÙˆØ±Ø©: {new_image}
"""
                    return {
                        "unit_type": "new_launch",
                        "message": formatted_details,
                        "details": {
                            "id": launch.get("id"),
                            "property_type_name": property_type_name,
                            "desc_ar": desc_ar,
                            "city_name": city_name,
                            "new_image": new_image
                        }
                    }
        except Exception as e:
            print(f"âš ï¸ Error loading from new_launches cache: {e}")
        
        return {
            "error": f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø¨Ø±Ù‚Ù… {unit_id}."
        }
        
    except Exception as e:
        print(f"ğŸš¨ Error in get_unit_details: {e}")
        return {
            "error": f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¹Ø±Ø¶ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙˆØ­Ø¯Ø©: {str(e)}"
        }



